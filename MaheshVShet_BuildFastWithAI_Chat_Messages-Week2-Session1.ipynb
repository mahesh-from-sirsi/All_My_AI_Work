{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mahesh-from-sirsi/All_My_AI_Work/blob/main/MaheshVShet_BuildFastWithAI_Chat_Messages-Week2-Session1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TplHAzzQ_k81"
      },
      "source": [
        "# Understanding Chat Messages\n",
        "\n",
        "\n",
        "- **System Message:** These are typically instructions or context provided to the AI system itself. They set up the framework for how the AI should operate, provide background information, or define parameters for the interaction.\n",
        "\n",
        "- **User Message:** This refers to the input or queries provided by the human user interacting with the AI system. It's the content that the AI needs to respond to or process.\n",
        "\n",
        "- **AI Message:** This is the response or output generated by the AI system based on the user's input and guided by the system message. It's the AI's contribution to the conversation or task."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "adDLNsW56YWS"
      },
      "source": [
        "### Basic Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "4Q2V8Z9VMm5A"
      },
      "outputs": [],
      "source": [
        "!pip install -qU langchain==0.3.4 langchain-openai==0.2.12 langchain-together==0.2.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cbA9knJEiPi9"
      },
      "source": [
        "- Get OpenAI API key: https://platform.openai.com/account/api-keys\n",
        "- Get Together AI API key: https://api.together.xyz/settings/api-keys"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Hf6OmfQjgEWm"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.environ['OPENAI_API_KEY'] = \"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "kopsuc39uhkD"
      },
      "outputs": [],
      "source": [
        "# Using OpenAI Models\n",
        "from langchain_openai import ChatOpenAI\n",
        "gpt4o_mini_model = ChatOpenAI(model = \"gpt-4o-mini\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Q2yjLXjDy0WN"
      },
      "outputs": [],
      "source": [
        "# Using a Llama Model with Together AI! [CLASSWORK]\n",
        "\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "llama_model = ChatOpenAI(model = \"meta-llama/Llama-3.2-90B-Vision-Instruct-Turbo\",\n",
        "                      openai_api_key = \"\", ## use your key\n",
        "                      openai_api_base = \"https://api.together.xyz/v1\"\n",
        "\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DrLW2TciEGop"
      },
      "source": [
        "### Using the API"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dmFJ9nAzGuWy",
        "outputId": "44cbbc4d-7acc-40e1-e077-ebf2cd4230c1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='Why did the scarecrow win an award? \\n\\nBecause he was outstanding in his field!', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 18, 'prompt_tokens': 12, 'total_tokens': 30, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_560af6e559', 'finish_reason': 'stop', 'logprobs': None}, id='run--b5d6fd40-ad99-473c-a8c4-e022aa6c1e64-0', usage_metadata={'input_tokens': 12, 'output_tokens': 18, 'total_tokens': 30, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "response = gpt4o_mini_model.invoke('Tell me a short joke')\n",
        "response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xqe7RIFQHBxo",
        "outputId": "86c87eaa-5694-4e30-9286-c68ea1f230e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Why did the scarecrow win an award? \n",
            "\n",
            "Because he was outstanding in his field!\n"
          ]
        }
      ],
      "source": [
        "print(response.content)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = llama_model.invoke('Tell me a short joke')\n",
        "print(response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        },
        "id": "GECMW7zex_eL",
        "outputId": "6cc6bd11-ea90-40c2-8e3e-3754af2769a9"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AuthenticationError",
          "evalue": "Error code: 401 - {'id': 'oB5uJMD-2kFHot-97e8d36c0c8506d0', 'error': {'message': 'Invalid API key provided. You can find your API key at https://api.together.ai/settings/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAuthenticationError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4050190163.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mllama_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Tell me a short joke'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    370\u001b[0m         return cast(\n\u001b[1;32m    371\u001b[0m             \u001b[0;34m\"ChatGeneration\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 372\u001b[0;31m             self.generate_prompt(\n\u001b[0m\u001b[1;32m    373\u001b[0m                 \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m                 \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36mgenerate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    955\u001b[0m     ) -> LLMResult:\n\u001b[1;32m    956\u001b[0m         \u001b[0mprompt_messages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_messages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprompts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 957\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt_messages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    958\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    959\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0moverride\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    774\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    775\u001b[0m                 results.append(\n\u001b[0;32m--> 776\u001b[0;31m                     self._generate_with_cache(\n\u001b[0m\u001b[1;32m    777\u001b[0m                         \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m                         \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36m_generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m   1020\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_from_stream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_generate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"run_manager\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1022\u001b[0;31m             result = self._generate(\n\u001b[0m\u001b[1;32m   1023\u001b[0m                 \u001b[0mmessages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_openai/chat_models/base.py\u001b[0m in \u001b[0;36m_generate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    687\u001b[0m             \u001b[0mgeneration_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"headers\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 689\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mpayload\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    690\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_chat_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgeneration_info\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    691\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/openai/_utils/_utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    284\u001b[0m                         \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"Missing required argument: {quote(missing[0])}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, prompt_cache_key, reasoning_effort, response_format, safety_identifier, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, verbosity, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m   1145\u001b[0m     ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[1;32m   1146\u001b[0m         \u001b[0mvalidate_response_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1147\u001b[0;31m         return self._post(\n\u001b[0m\u001b[1;32m   1148\u001b[0m             \u001b[0;34m\"/chat/completions\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1149\u001b[0m             body=maybe_transform(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1257\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mto_httpx_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1258\u001b[0m         )\n\u001b[0;32m-> 1259\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mResponseT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1261\u001b[0m     def patch(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1045\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1046\u001b[0m                 \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Re-raising status error\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1047\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_status_error_from_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1048\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1049\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAuthenticationError\u001b[0m: Error code: 401 - {'id': 'oB5uJMD-2kFHot-97e8d36c0c8506d0', 'error': {'message': 'Invalid API key provided. You can find your API key at https://api.together.ai/settings/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pYMpb5gDNVG3"
      },
      "source": [
        "### Human and AI Messages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DrR-jRbbHcAB",
        "outputId": "4855c007-2d7d-4dcc-db97-d88d3be567b2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='Why did the scarecrow win an award? \\n\\nBecause he was outstanding in his field!', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 18, 'prompt_tokens': 12, 'total_tokens': 30, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_560af6e559', 'finish_reason': 'stop', 'logprobs': None}, id='run--a932622c-d62f-48aa-b1a8-2cf2a9a03d3c-0', usage_metadata={'input_tokens': 12, 'output_tokens': 18, 'total_tokens': 30, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "from langchain_core.messages import HumanMessage\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "gpt4o_mini_model = ChatOpenAI(model=\"gpt-4o-mini\")\n",
        "\n",
        "messages = [HumanMessage(content=\"Tell me a short joke\")]\n",
        "\n",
        "response = gpt4o_mini_model.invoke(messages)\n",
        "\n",
        "response"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3nbg0NWjNMuh"
      },
      "source": [
        "### Working with System Prompts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5RxZl4RXQlen"
      },
      "source": [
        "\n",
        "```python\n",
        "System messages in AI conversations provide essential context and instructions to guide the AI behavior.\n",
        "They can be categorized into four types:\n",
        "\n",
        "\n",
        "Role definitions: These establish the AI's persona or expertise.\n",
        "Example: \"You are a professional chef specializing in Italian cuisine.\"\n",
        "\n",
        "Task-specific instructions: These outline the specific actions or approaches the AI should take.\n",
        "Example: \"Provide step-by-step coding solutions with explanations for each step.\"\n",
        "\n",
        "Behavioral guidelines or constraints: These set rules for how the AI should communicate or what it should avoid.\n",
        "Example: \"Always maintain a formal and professional tone in your responses.\"\n",
        "\n",
        "Background information: This provides context or knowledge relevant to the conversation.\n",
        "Example: \"You are assisting users on a website for a museum of modern art in New York City.\"\n",
        "\n",
        "By using these types of system messages, developers can customize AI behavior to suit various applications and user needs.\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "a5fGUN2wEGor",
        "outputId": "3d7dee95-5369-46fe-fa63-7ef1aae38d28",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='I’m called ChatGPT. How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 13, 'prompt_tokens': 12, 'total_tokens': 25, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_560af6e559', 'finish_reason': 'stop', 'logprobs': None}, id='run--509f0789-190a-4a2d-8262-5c23f42b5ef1-0', usage_metadata={'input_tokens': 12, 'output_tokens': 13, 'total_tokens': 25, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "from langchain_core.messages import HumanMessage, SystemMessage\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "gpt4o_mini_model = ChatOpenAI(model=\"gpt-4o-mini\")\n",
        "\n",
        "messages = [\n",
        "    HumanMessage(content=\"What is your name?\")\n",
        "]\n",
        "\n",
        "response = gpt4o_mini_model.invoke(messages)\n",
        "\n",
        "response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "SOjjuBSxHBve",
        "outputId": "165c7887-1fb8-4de6-d6ba-27191997030a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='My name is Bob! How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 12, 'prompt_tokens': 27, 'total_tokens': 39, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_560af6e559', 'finish_reason': 'stop', 'logprobs': None}, id='run--7f638ca7-9f49-41f7-92d4-df81cabebb07-0', usage_metadata={'input_tokens': 27, 'output_tokens': 12, 'total_tokens': 39, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "from langchain_core.messages import HumanMessage, SystemMessage\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "gpt4o_mini_model = ChatOpenAI(model=\"gpt-4o-mini\")\n",
        "\n",
        "messages = [\n",
        "    SystemMessage(content=\"You are a helpful assistant! Your name is Bob.\"),\n",
        "    HumanMessage(content=\"What is your name?\")\n",
        "]\n",
        "\n",
        "response = gpt4o_mini_model.invoke(messages)\n",
        "\n",
        "response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v_kskcxSHBtH",
        "outputId": "afdf2e88-517a-466c-e5dd-03e7543f2e34"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "My name is Bob! How can I assist you today?\n"
          ]
        }
      ],
      "source": [
        "print(response.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h-wNHzpDHBqb",
        "outputId": "e23b14be-c4b3-41ba-e021-b61317f075b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Designing a smart water bottle requires a blend of aesthetic appeal, functionality, and user experience. Here’s a structured approach to conceptualizing a smart water bottle:\n",
            "\n",
            "### 1. **User Research**\n",
            "   - **Target Audience**: Identify who will use the smart water bottle (e.g., athletes, office workers, outdoor enthusiasts).\n",
            "   - **Needs Assessment**: Conduct surveys or interviews to understand user pain points with current hydration solutions. \n",
            "\n",
            "### 2. **Functionality**\n",
            "   - **Hydration Tracking**: Integrate sensors to monitor water intake and sync with a mobile app.\n",
            "   - **Reminders**: Incorporate customizable notifications to remind users to drink water.\n",
            "   - **Temperature Control**: Use double-walled insulation to maintain the temperature of beverages.\n",
            "   - **Self-Cleaning Feature**: Consider UV-C light technology for self-cleaning capabilities.\n",
            "   - **Customization**: Allow users to set personal hydration goals and preferences through the app.\n",
            "\n",
            "### 3. **Aesthetic Design**\n",
            "   - **Material Selection**: Use high-quality, sustainable materials like stainless steel or BPA-free plastics.\n",
            "   - **Shape and Size**: Design a sleek, ergonomic shape that fits comfortably in hand and cup holders.\n",
            "   - **Color Palette**: Offer a range of colors that appeal to different demographics, including neutral tones and vibrant hues.\n",
            "   - **Minimalist Approach**: Embrace a minimalist design with subtle branding to maintain a sophisticated look.\n",
            "\n",
            "### 4. **User Interface**\n",
            "   - **Intuitive App Design**: Create a user-friendly app interface that provides feedback on hydration levels, goal progress, and trends.\n",
            "   - **Physical Interaction**: Consider touch-responsive buttons or a touch screen on the bottle for immediate interaction.\n",
            "\n",
            "### 5. **Sustainability**\n",
            "   - **Eco-Friendly Materials**: Use recyclable or biodegradable materials to minimize environmental impact.\n",
            "   - **Refillable and Durable**: Design the bottle to be durable and encourage users to refill rather than buy single-use plastic bottles.\n",
            "\n",
            "### 6. **Testing and Feedback**\n",
            "   - **Prototyping**: Create several prototypes to test design, functionality, and user experience.\n",
            "   - **User Testing**: Gather feedback from potential users and iterate on the design based on their input.\n",
            "\n",
            "### 7. **Marketing and Branding**\n",
            "   - **Positioning**: Highlight the unique features of the smart water bottle in marketing campaigns.\n",
            "   - **Storytelling**: Craft a narrative around the importance of hydration and how the product fits into a healthy lifestyle.\n",
            "\n",
            "### 8. **Launch and Beyond**\n",
            "   - **Community Building**: Create a community around hydration through social media, contests, and user-generated content.\n",
            "   - **Continuous Improvement**: Gather user feedback post-launch to iterate on features and design for future versions.\n",
            "\n",
            "By focusing on these aspects, we can create a smart water bottle that not only meets functional needs but also resonates aesthetically with users, encouraging them to stay hydrated in style.\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.messages import HumanMessage, SystemMessage\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "messages = [\n",
        "    SystemMessage(content=\"You are a former Chief Design Offer at Apple. You help companies build aesthetic product.\"),\n",
        "    HumanMessage(content=\"How can you help design a smart water bottle?\"),\n",
        "]\n",
        "\n",
        "response = gpt4o_mini_model.invoke(messages)\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "WPX_7P79HBkf",
        "outputId": "33aa954e-9431-43ee-8d86-dd074c6cabb4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I’m Zoya, an AI developed by Build Fast with AI. I'm here to assist you with information, answer questions, and help with various tasks. How can I assist you today?\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.messages import HumanMessage, SystemMessage\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "messages = [\n",
        "    SystemMessage(content=\"You are a Zoya, a bot built by Build Fast with AI.\"),\n",
        "    HumanMessage(content=\"Who are you?\"),\n",
        "]\n",
        "\n",
        "response = gpt4o_mini_model.invoke(messages)\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "PxfEjDXfHBhV",
        "outputId": "9e333093-da12-4bd4-f103-5f06026bba14",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "World War I ended on November 11, 1918. The armistice was signed at 11 a.m. Paris time, marking the cessation of hostilities on the Western Front. The official end of the war was later formalized with the Treaty of Versailles, signed on June 28, 1919.\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.messages import HumanMessage, SystemMessage\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "messages = [\n",
        "    SystemMessage(content=\"You are a Zoya, a bot built by Build Fast with AI.\"),\n",
        "    HumanMessage(content=\"When did world war 1 end?\"),\n",
        "]\n",
        "\n",
        "response = gpt4o_mini_model.invoke(messages)\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "sG7atNvpHBek",
        "outputId": "74d76975-b152-4bcc-df67-d9a111dea4d4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I'm here to help with questions related to AI. If you have any inquiries about AI technologies, applications, or concepts, feel free to ask!\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.messages import HumanMessage, SystemMessage\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "messages = [\n",
        "    SystemMessage(content=\"You are a Zoya, a bot built by Build Fast with AI. You don't answer any questions not related to AI.\"),\n",
        "    HumanMessage(content=\"When did world war 1 end?\"),\n",
        "]\n",
        "\n",
        "response = gpt4o_mini_model.invoke(messages)\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "KmGXy7zTEGot",
        "outputId": "2fe63af2-c502-493c-9e35-b1efe895cea9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The transformer architecture is a deep learning model introduced in the paper \"Attention is All You Need\" by Vaswani et al. in 2017. It has become the foundation for many state-of-the-art models in natural language processing (NLP) and beyond. Here are the key components and concepts of the transformer architecture:\n",
            "\n",
            "1. **Self-Attention Mechanism**: The core innovation of the transformer is the self-attention mechanism, which allows the model to weigh the importance of different words in a sentence when encoding them. It computes attention scores for each word with respect to every other word, enabling the model to capture contextual relationships effectively.\n",
            "\n",
            "2. **Multi-Head Attention**: Instead of having a single attention mechanism, transformers use multiple heads to learn different representations of the input. Each head independently computes attention scores and produces a set of output vectors, which are then concatenated and linearly transformed.\n",
            "\n",
            "3. **Positional Encoding**: Since transformers do not have a built-in notion of word order (unlike recurrent neural networks), they use positional encodings to inject information about the position of words in the sequence. These encodings are added to the input embeddings.\n",
            "\n",
            "4. **Feed-Forward Neural Networks**: After the attention layers, the output is passed through feed-forward neural networks (FFNNs) that apply a non-linear transformation to each position independently.\n",
            "\n",
            "5. **Layer Normalization and Residual Connections**: Each sub-layer (self-attention and feed-forward) is followed by layer normalization and a residual connection, which helps in stabilizing training and allows gradients to flow more easily through the network.\n",
            "\n",
            "6. **Stacking Layers**: The transformer consists of an encoder and a decoder, each made up of a stack of identical layers. The encoder processes the input sequence, while the decoder generates the output sequence, making use of the encoder's output.\n",
            "\n",
            "7. **Encoder-Decoder Structure**: In the full transformer architecture, the encoder takes the input sequence and transforms it into a set of continuous representations. The decoder then takes these representations and generates the output sequence, often used for tasks like translation.\n",
            "\n",
            "8. **Training with Attention Masking**: During training, particularly in the decoder, attention masking is used to prevent the model from attending to future tokens, ensuring that predictions for a position can only depend on known outputs at earlier positions.\n",
            "\n",
            "Transformers have been widely adopted beyond NLP, including in image processing and other domains, due to their ability to handle long-range dependencies and parallelize training effectively.\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.messages import HumanMessage, SystemMessage\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "messages = [\n",
        "    SystemMessage(content=\"You are a Zoya, a bot built by Build Fast with AI. You don't answer any questions not related to AI.\"),\n",
        "    HumanMessage(content=\"Explain transformer architecture.\"),\n",
        "]\n",
        "\n",
        "response = gpt4o_mini_model.invoke(messages)\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lluhu6l9EGot"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q17DvZu_EGot"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}