{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mahesh-from-sirsi/All_My_AI_Work/blob/main/MaheshVShet_BuildFastWithAI_Module2_5_Chatbot_on_Any_Data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cIpHbuJaZaFF"
      },
      "source": [
        "# Chatbot with Website/YouTube Video\n",
        "This guide will walk you through creating a Question-Answering system for Website/YT Video documents using Retrieval-Augmented Generation (RAG) with Langchain and Pinecone."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9E8hI_p4D_OT"
      },
      "source": [
        "### Installing Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "NL2mjFUDDJOg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab1b2387-116c-48a0-c34c-ca4bbc23465c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/67.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m37.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.0/75.0 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.8/19.8 MB\u001b[0m \u001b[31m67.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.2/284.2 kB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m53.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m447.5/447.5 kB\u001b[0m \u001b[31m25.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.3/103.3 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.5/16.5 MB\u001b[0m \u001b[31m74.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m131.9/131.9 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.7/65.7 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m208.0/208.0 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.6/71.6 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m510.8/510.8 kB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m80.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m452.2/452.2 kB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "%pip install -qU langchain-community  langchain langchain-openai requests chromadb beautifulsoup4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EQvnv-65EHnD"
      },
      "source": [
        "### Storing API keys"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "elq0lB-KZfZ4"
      },
      "source": [
        "- Get OpenAI API key: https://platform.openai.com/account/api-keys"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "x1_dkxdWkm4a"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AOV3jZolLeOo"
      },
      "source": [
        "## Chat with Website Using ChromaDB\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MtnhSnxsMPYW"
      },
      "source": [
        "### Import Required Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "jAFvzqyFfEgo",
        "outputId": "4e5c364c-cda8-4aa2-f9ff-11f31d6d8471",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_community.utils.user_agent:USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
          ]
        }
      ],
      "source": [
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain.memory import ConversationBufferMemory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BjFlql1LLp55"
      },
      "source": [
        "### Load Website Content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "z0u8J58xLsGB"
      },
      "outputs": [],
      "source": [
        "def load_website(url):\n",
        "    loader = WebBaseLoader(url)\n",
        "    data = loader.load()\n",
        "    return data\n",
        "\n",
        "# Example usage\n",
        "url = \"https://www.buildfastwithai.com/\"  # Replace with your target website\n",
        "website_data = load_website(url)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8xMFN4D4L0Kq",
        "outputId": "438d4351-e1ae-4585-ed31-a6dbbecd5eaf"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'source': 'https://www.buildfastwithai.com/', 'title': 'Build Fast with AI', 'description': 'Build Fast with AI - a vibrant community of AI builders, innovators, and enthusiasts. Whether you are an entrepreneur, a product manager, a developer, or anyone intrigued by AI, this is your platform to learn, grow, and innovate.', 'language': 'en'}, page_content='Build Fast with AIGenAI BootcampCorporate TrainingAI WorkshopsBuildFast StudioMore Sign In Sign In Ask toBuildFast BotHey! Wanna know about Generative AI Crash Course?What will I learn?How can I join?What\\'s the course duration?What\\'s the course fee?What\\'s the course syllabus?SendGenAI 2025 Launch PadTransform AI Ideas into RealityJoin 20,000+ professionals mastering practical AI development at Build Fast with AILearn. Build. Deploy.Begin Your AI JourneyJoin WaitlistNow Partnering with Industry LeadersWe don‚Äôt just build with AI. We build with people.Partnering with TWO AILeading DevRel for SUTRALeading developer growth for SUTRA, TWO AI‚Äôs multilingual GenAI model by Pranav Mistry, supporting 50+ Indian languages to advance inclusive tech.xto10x CollaborationExclusive GenAI Training for xto10xCollaborated with xto10x on a 4-week GenAI sprint, guiding startups from concepts to deployed AI tools, ensuring practical application and impact.What We OfferWhere Curiosity meets capability ‚Äì AI learning that actually sticks.Gen AI LaunchpadAn 8-week course with comprehensive lectures and hands-on projects, taught by our founder Satvik. Learn to build with AI from scratch - no prerequisites required.Learn MoreCorporate TrainingUpskill your team with practical AI expertise. We deliver tailored, hands-on training that help your people solve real problems with AI.Learn MoreAI ConsultingWe integrate AI seamlessly into your existing business. From strategy to implementation, we focus on solutions that create measurable impact, not just ideas on paper.Learn MoreNEW!Generative AI Launch Pad 2025Ready to become a Generative AI developer? Join our intensive bootcamp where you\\'ll learn to build and deploy cutting-edge AI applications. From LLMs to RAG, from prototypes to production - we\\'ve got you covered.20,000+Learners Trained400+WorkshopsJoin WaitlistView CurriculumYour Instructor: Satvik ParamkushamAI Developer & Consultant ‚Ä¢ IIT Delhi Alumni AI AcceleratorsSignature Events ShowcasePrevious slideNext slideGoogle\\'s : Build with Gen AI WorkshopDevelopers explored cutting-edge AI tools, got hands-on with Gemini models, and networked with industry innovators at Google\\'s Bangalore office.AI Meets ProductPMs explored how AI reshaped product development, discussed evolving roles, and connected with Bangalore\\'s tech elite in this insightful meetup.2024 GenAI Half Yearly RewindAttendees unpacked the year\\'s groundbreaking AI developments, gained insights from AI pioneers, and learned practical applications across various fields.The Product Game ChangerPMs explored AI\\'s impact on product management, discussed transforming PM functions, demystified AI concepts, analyzed AI-driven strategies, & identified future-ready PM skills in this innovative Bangalore coffee meetup.Build Fast with AI x Mesa School of BusinessMesa Business Students participated in a 12 hour Gen AI to create 5-minute movies from scratch, completely using Gen AI tools! No coding, just creativity and AI magic! \\uf8ffüé•‚ú®Build Fast with AI x BHIVEParticipants brought their product ideas to life using tools like Bolt.new and Lovable. A hands-on session on building fast without writing code.AI Prototyping for PMsA hands-on meetup designed for Product Managers to turn ideas into working AI tools without writing code. Focused on speed, collaboration, and building real solutions fast.AI Product Building WorkshopA live session where founders turned raw ideas into real tools on the spot. The event focused on speed, simplicity, and getting things out there without overthinking.Our Learners Work at World-Class CompaniesWorkshops0+Professionals Trained0+Companies Trust Us0+Upcoming EventsJoin our community events and connect with like-minded peopleFree Workshop LibraryA growing library of our webinars, available anytime.Previous slideNext slideFreeAI Product Building Workshop: From Zero to LiveSatvik, founder of Build Fast with AI, introduced the concept of \"Vibe coding\" - using AI tools to build websites and applications with minimal human coding effort - demonstrating how to create everything from simple portfolio websites to complex project management tools using platforms like Plot AI, Gemini Canvas, and V0. He explained the three levels of no-code AI development and showcased how these tools are drastically changing software development by enabling one person to accomplish in a day what previously required multiple specialists working for months.Start LearningFreeMake.com AI Automation MasterclassSatvik, the founder of Build Fast with AI, led a comprehensive workshop on AI automation, demonstrating various tools and techniques for automating work processes and explaining the capabilities and limitations of different AI systems. He presented a new 8-week generative AI course designed to help participants gain practical AI knowledge and skills, emphasizing the importance of building a solid foundation before specializing in specific roles. The session concluded with a demonstration of various AI automation tools and a discussion on setting a career path in AI, highlighting the course\\'s affordability and accessibility for individuals looking to upskill in the field.Start LearningFreeBuild AI AutomationsSatvik, the founder of Build Fast with AI, conducted a workshop on automating tasks using various AI tools and demonstrated practical applications like automated PowerPoint presentations and personalized email creation. He explained the differences between traditional rule-based automation and generative AI, showcasing examples of agentic automations and AI research agents that can perform complex tasks like buyer persona analysis and deep research. The session concluded with a discussion on the impact of generative AI on the job market and the introduction of a comprehensive 8-week Generative AI Launchpad course, which Satvik presented with details about pricing and discounts.Start LearningFreeCursor for Non-Coders: Build Your First AI Agent in 90 MinutesSatvik, the founder of Build Fast with AI, conducted a workshop on using AI coding tools like Cursor and Gemini to help beginners create websites and applications. He demonstrated various techniques and tools for building websites, including vibe coding and the use of AI models to generate code, while explaining how AI can significantly reduce development time and make coding accessible to non-technical users. The session concluded with information about an upcoming 8-week generative AI course, including pricing details and a demonstration of the Cursor AI tool for coding, along with a brief discussion about AI security and course registration promotions.Start LearningFreeBuild Your First AI AgentSatvik, the founder of Build Fast with AI, introduced a workshop focused on teaching practical AI agent and automation building, emphasizing the gap between AI advancements and real-world application. Through demonstrations and explanations, he covered topics including generative AI for email automation, the limitations of ChatGPT, and the evolution of automation from rule-based systems to AI-driven solutions. The session concluded with details about an upcoming generative AI course, including its curriculum, pricing, and tools, while also addressing questions about AI\\'s impact on jobs and strategies for humanizing AI-generated content.Start LearningFreeBuild Your AI Stock Market AgentSatvik conducted a comprehensive workshop on AI tools for finance and data analysis, sharing his background and introducing the company\\'s offerings including workshops, consulting, and courses. He demonstrated various AI applications for financial analysis, market research, and portfolio tracking, showcasing how these tools can transform traditional finance tasks and generate insights through data visualization and reporting. The workshop concluded with an announcement of an upcoming generative AI course starting July 5th, offering discounts and covering applied AI concepts through live sessions and video lectures.Start LearningBrowse All ResourcesHear from the CommunityPrevious slideNext slideThe best AI course that brings you up to speed even if you are a beginner and places you at the cutting edge from where it is your responsibility to stay updated. The best decision I took to enrol for this course and well worth the money.George KoshyBFSI Head, Predoole AnalyticsSatvik is a patient, conscientious and well intentioned teacher who is adept and up to date on innovations occurring in the GenAI space. What makes him stand out is that he goes out of his way to assist students in answering their queries even outside class, at the expense of his spare time.Adit ParekhProduct Strategy, FreshworksAs a non-technical co-founder I had taken this course in the 1st cohort and it was very good, gave a comprehensive overview of all the important aspects of GenAI and we also got hands-on practice. I liked it so much that our CTO joined the second cohort and he also found it very useful.Nihal KashinathFounder and CEO, Deeptech StarsAs an educator, I am always on the lookout for courses that can help my students stay updated with the latest technological advancements. This course is one of them, the hands-on projects, such as the story-telling app and YouTube bot, were not only educational but also fun and creative.Arti KalroProfessor, IIT BombayThe Crash Course on Generative AI by Build Fast with AI is an exceptional program for anyone looking to dive into the world of AI. Satvik\\'s teaching style is engaging and easy to follow, even for those without a technical background. I particularly appreciated the hands-on projects that allowed us to apply our learning in real-world scenarios.Abhishek GopalkaManaging Director, BCGI recently completed this course as a network engineer, and I\\'m genuinely impressed by how it\\'s transformed my perspective on Generative AI in our field. I\\'m particularly excited to apply these newfound skills to enhance our cloud and network infrastructure management.Amrinder RandhawaNetwork Engineer, GoogleSatvik is one of few people who have been building, educating and delivering great sessions on AI for more than a year. The \\'learn by doing\\' approach was invaluable, it has helped us visualize and understand the key topics in a very short span of time.Sriharsha VellankiProduct Leader, Zee5I would like to extend my heartfelt gratitude to Satvik Paramkusham and their dedicated team for delivering such an outstanding course. Their hands-on exercises and unwavering support for any questions or issues made the learning experience truly exceptional.Ramesh NookalaSenior Director, SAP AribaGreat course to start your journey into GenAI. Currently playing the role of Agility Coach in Commonwealth Bank but this course is for anyone who wants to learn more about GenAI and figure out how it can help in your role and make your life easy.Sahil JainProject Manager, Commonwealth Bank of AustraliaThe course was an eye-opener into the fascinating world of Generative AI. Satvik\\'s deep knowledge and passion for the subject made the learning experience incredibly engaging and insightful. The hands-on approach and real-world applications discussed in the program helped me grasp complex concepts with ease.Sumit MalhotraChief of Staff, Iron PillarThe course was super helpful! Everything was explained clearly, in details, and the examples were very practical. I created my first AI projects, including my own chatbot, and I\\'m planning to use AI in other projects as well.Yuliia KutsFull-Stack and Data EngineerAfter exploring several GenAI courses‚Äîboth recorded and instructor-led‚ÄîI can confidently say this has been the most impactful one. Coming in with just a basic understanding of Python, I was able to build and deploy mini GenAI projects within just six weeks.Nanda KumarSenior Manager, ChargePointReady to Begin Your AI Journey?Register now and upgrade your skills with our AI courses & events.Register Nowsatvik@buildfastwithai.comKoramangala, Bengaluru, 560034SupportGenAI CourseBuildFast StudioCompanyResourcesEventsLegalPrivacyTermsRefundOur ProductsEduchainAI-powered education platform for teachersBuildFast StudioThe Indian version of CharacterAI but even more varieties.LinkedInInstagramTwitterGitHub¬© 2025 Intellify Edventures Private Limited All rights reserved.')]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "website_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fpy5pNbaL36b"
      },
      "source": [
        "### Split Content into Chunks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "jS0rB4ouL3Yj"
      },
      "outputs": [],
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "splits = text_splitter.split_documents(website_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "iQ-4MLpmZfZ-",
        "outputId": "81dd45f4-03c9-48de-82bd-d6bac1026e0b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "16"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "len(splits)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6HF4ye1lL8fD"
      },
      "source": [
        "### Initialize Embeddings and Chroma Vector Store"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "-IW_WCKiL_dF"
      },
      "outputs": [],
      "source": [
        "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
        "\n",
        "# Create a Chroma vector store\n",
        "vectorstore = Chroma.from_documents(splits, embeddings)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qz2_CxAJMYyo"
      },
      "source": [
        "### Set Up Conversational Retrieval Chain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "gXDTAXW0Mdco",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24822d26-71c7-4b86-cf18-a3d6d8555579"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3162655233.py:6: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
            "  memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n"
          ]
        }
      ],
      "source": [
        "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0.7)\n",
        "\n",
        "qa = ConversationalRetrievalChain.from_llm(\n",
        "    llm=llm,\n",
        "    retriever=vectorstore.as_retriever(),\n",
        "    memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
        "\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YGfmcvGQMvta"
      },
      "source": [
        "### Chat Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "4sS6R5jbMx0K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "18fa231e-fc66-498e-e564-1928802dec00"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Human: What is the main topic of this website?\n",
            "AI: The main topic of the website is focused on AI tools and automation, particularly in the context of building applications and websites with minimal coding. It offers workshops, courses, and resources for learning AI automation, generative AI, and using AI tools for tasks like coding, finance, and data analysis. The website appears to aim at making AI technology accessible to both technical and non-technical users, with an emphasis on practical applications and skill-building.\n"
          ]
        }
      ],
      "source": [
        "def chat_with_website(query):\n",
        "    result = qa.invoke({\"question\": query})\n",
        "    return result['answer']\n",
        "\n",
        "# Example usage\n",
        "query = \"What is the main topic of this website?\"\n",
        "response = chat_with_website(query)\n",
        "print(f\"Human: {query}\")\n",
        "print(f\"AI: {response}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "1p0xIv0RZfZ_",
        "outputId": "ee241190-642b-4a7d-b203-caf1e66e2523",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Human: Tell me about the Generative AI Bootcamp\n",
            "AI: The Generative AI Bootcamp offered on the website is an intensive program designed to help you become a Generative AI developer. The bootcamp covers building and deploying cutting-edge AI applications, from large language models (LLMs) to retrieval-augmented generation (RAG), and takes projects from prototypes to production. The program includes comprehensive lectures and hands-on projects, and it is taught by the founder, Satvik Paramkusham, an AI Developer & Consultant and IIT Delhi alumnus. There are no prerequisites required to join the course.\n"
          ]
        }
      ],
      "source": [
        "# Example usage\n",
        "query = \"Tell me about the Generative AI Bootcamp\"\n",
        "response = chat_with_website(query)\n",
        "print(f\"Human: {query}\")\n",
        "print(f\"AI: {response}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "nnP3nZcEZfaA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f53e2caf-f4d1-4970-a6b4-4a9661c41d10"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Human: Who is the author of this website and who is the mentor\n",
            "AI: The author of the website focused on AI tools and automation, and who serves as the mentor for the Generative AI Bootcamp is Satvik, the founder of Build Fast with AI.\n"
          ]
        }
      ],
      "source": [
        "# Example usage\n",
        "query = \"Who is the author of this website and who is the mentor\"\n",
        "response = chat_with_website(query)\n",
        "print(f\"Human: {query}\")\n",
        "print(f\"AI: {response}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kdw9Q5lsNm3Z"
      },
      "source": [
        "## Chat with YouTube Video Using ChromaDB"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qvf3PErSNyl8"
      },
      "source": [
        "### Install the required dependencies:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %pip install -qU langchain-community langchain langchain-openai requests chromadb beautifulsoup4"
      ],
      "metadata": {
        "id": "3_C0PXi1hDRU"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "NxIQar_0NsVj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4510a0b-26f1-49b1-a322-0481d9341570"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/485.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m481.3/485.0 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m485.0/485.0 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/57.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -qU langchain-community langchain langchain-openai requests chromadb youtube_transcript_api pytube"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## IMPORTS OF THE PREVIOUS SECTION\n",
        "\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "Z04b57pnhZbW"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7S5pKUk0N2PL"
      },
      "source": [
        "### Import Required Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "32wLDeKCN5GF"
      },
      "outputs": [],
      "source": [
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
        "from langchain_community.document_loaders import YoutubeLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from youtube_transcript_api import YouTubeTranscriptApi #\n",
        "from langchain.schema import Document                   #\n",
        "import re                                               #"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JhelKkklN898"
      },
      "source": [
        "### Load Video Transcript"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "8GVm7XWlN-Nl"
      },
      "outputs": [],
      "source": [
        "def load_video_transcript(video_url):\n",
        "    def extract_video_id(url):\n",
        "        patterns = [\n",
        "            r'(?:youtube\\.com\\/watch\\?v=|youtu\\.be\\/|youtube\\.com\\/embed\\/)([^&\\n?#]+)',\n",
        "            r'youtube\\.com\\/watch\\?.*v=([^&\\n?#]+)',\n",
        "        ]\n",
        "        for pattern in patterns:\n",
        "            match = re.search(pattern, url)\n",
        "            if match:\n",
        "                return match.group(1)\n",
        "        return None\n",
        "\n",
        "    video_id = extract_video_id(video_url)\n",
        "    api = YouTubeTranscriptApi()\n",
        "    transcript_data = api.fetch(video_id)\n",
        "\n",
        "    # Access snippets correctly\n",
        "    full_text = \" \".join([snippet.text for snippet in transcript_data.snippets])\n",
        "\n",
        "    return [Document(\n",
        "        page_content=full_text,\n",
        "        metadata={\n",
        "            'source': video_url,\n",
        "            'video_id': video_id,\n",
        "            'language': transcript_data.language_code\n",
        "        }\n",
        "    )]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "3r5z4V7kOqkb"
      },
      "outputs": [],
      "source": [
        "# Example usage\n",
        "# video_url = \"https://www.youtube.com/watch?v=hzUuklUo5NA\"  # Replace with your target video\n",
        "# video_data = load_video_transcript(video_url)\n",
        "\n",
        "\n",
        "video_url = \"https://www.youtube.com/watch?v=bCz4OMemCcA\"  # Replace with your target video\n",
        "video_data = load_video_transcript(video_url)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "video_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AqoeejG_cPR1",
        "outputId": "c04e5b8c-3f5a-44f0-babc-8ba60d6696cc"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'source': 'https://www.youtube.com/watch?v=bCz4OMemCcA', 'video_id': 'bCz4OMemCcA', 'language': 'en'}, page_content=\"hello guys welcome to my video about the Transformer and this is actually the person 2.0 of my series on the Transformer I had a previous video in which I talked about the Transformer but the audio quality was not good and as suggested by my viewers as the video was really uh had a huge success the viewers suggested me to to improve their audio quality so this this is why I'm doing this video uh you don't have to watch the previous series because I would be doing basically the same things but with some improvements so I'm actually compensating from some mistakes I made or from some improvements that I could add after watching this video I suggest watch my watching my other video about or how to code a Transformer model from scratch so how to code the model itself how to train it online data and how to inference it stick it with me because it's gonna be a little long journey but for sure what now before we talk about the Transformer I want to first talk about recurrent neural networks so the networks that were used before they introduced the transformer for most of the sequence to sequence jobs tasks so let's review them recurring neural networks existed a long time before the Transformer and they allowed to map one sequence of input to another sequence of output in this case our input is X and we want an input sequence Y what we did before is that we split the sequence into single items so we gave the recurrent neural network the first item as input so X1 along with an initial State usually made up of only zeros and the recurrent normal Network produced an output let's call it y1 and this happened at the first time step then we took the hidden State this is called the hidden state of the network of the previous time step along with the next input token so X2 and the network had to produce the SEC the second output token Y2 and then we did it the same procedure at the third time step in which we took the hidden state of the previous time step along with the input State the input token at the time steps 3 and the network has to produce the next output token which is Y3 if you have enter n tokens you need n time steps to map a end sequence input into an end sequence output this worked fine for a lot of tasks but had some problems let's review them the problems with recurring neural networks first of all are that they are slow for long sequences because think of the process we did before we have kind of like a for Loop in which we do the same operation for every token in the input so if you have the longer the sequence the longer this computation and this made the the network not easy to train for long sequences the second problem was the vanishing or the exploding gradients now you may have heard these terms or expression on the Internet or from other videos but I will try to give you a brief Insight on what does what do they mean on a practical level so as you know Frameworks like Pi torch they convert our networks into a computation graph so basically suppose we have a computation graph I this is not an error network I will making I will be making a computational graph that is very simple has nothing to do with the neural networks but will show you the problems that we have so imagine we have two inputs X and another input let's call it y our computational graph first let's say multiplies these two numbers so we have a first a function let's call it f of x and y that is X multiplied by y let me multiplied and the result let's call it Z is map is given to another function let's call this function G of Z is equal to let's say Z squared what our phytorch for example does it's that pytorch want to calculate the usually we have a loss function by torch calculates the derivative of the loss function with respects to its each weight in this case we just calculate the derivative of the G function so the output function with respect to all of its inputs so derivative of G with respect to X let's say is equal to the derivative of G with respect to f and multiplied by the derivative of f with respect to X these two should kind of cancel out this is called the chain Rule now as you can see the longer the chain of computation so if we have many nodes one after another the longer this multiplication chain so here we have two because the distance from this node and this is two but imagine you have 100 or 1000 now imagine this number is 0.5 and this number is 0.5 also the resulting numbers when multiplied together is a number that is smaller than the two initial numbers it's gone up 0.25 because it's one to one half multiplied by one half is one fourth so if we have two numbers that are smaller than one and we multiply them together they will produce an even smaller number and if we have two numbers that are bigger than one and we multiply them together they will produce a number that is bigger than both of them so if we have a very long chain of computation it eventually will either become a very big number or a very small number and this is not desirable first of all because our CPU of our GPU can only represent numbers up to a certain Precision let's say 32-bit or 64-bit and if the number becomes too small the contribution of this number to the output will become very small so when the pi torch or our automatic let's say our framework will calculate how to adjust the weights the weight will move very very very slowly because the contribution of this product is will be a very small number and this means that we have the gradient is Vanishing or in the other case it can explode become very big numbers and this is a problem the next problem is difficulty in accessing information from long time ago what does it mean it means that as you remember from the previous slide we saw that the first input token is given to the recurrent neural network to with along with the first state now we need to think that the recurrent neural network is a long graph of computation it will produce a new hidden State then we will use the the new hidden State along with the next token to produce the next output if we have a very long sequence um of input sequence the last token will have a hidden state whose contribution from the first token has nearly gone because of this long chain of multiplication so actually the last token will not depend much on the first token and this is also not good because for example we know as humans that in a text in a quite long text the context that we saw let's say 200 words before still relevant to the context of the current words and this is something that the RNN could not map and this is why we have the Transformer so the Transformer solves these problems with the recurrent neural networks and we will see how the structure of the Transformer we can divide into two macro blocks the first macro block is called encoder and it's this part here the second macro block is called a decoder and it's the second part here the third part here you see on the top it's just a linear layer and we will see why it's there and what it is function so and the two layers so the encoder and the decoder are connected by this connection you can see here in which some output of the encoder is sent as input to the decoder and we will also see how let's start first of all with some notations that I will be using during my explanation and you should be familiar with this notation also to review some maths so the first thing we should be familiar with is matrix multiplication so imagine we have a input Matrix which is a sequence of let's say words so sequence by D model and we will see why it's called sequence by the model so imagine we have a matrix that is a 6 by 512 in which each row is a word and this word is not made of characters but by 512 numbers so each word is represented by 512 numbers okay like this imagine you have 512 of them along this row 512 along this other row etc etc one two three four five so we need another one here okay the first word we will call it a the second B the C D E and F if we multiply this matrix by another Matrix let's say the transpose of this Matrix so it's a matrix where the rows becomes columns so three four five and six this word will be here B C D E and F and then we have um 512 numbers along each column because before we had them on the rows now they will become on the column so here we have the 512 number etc etc this is a matrix that is 512 by 6 so let me add some brackets here if we multiply them we will get a new Matrix that is we cancel the inner dimensions and we get the outer Dimension so it will become six by six so it will be 6 rows by 6 rows so let's draw it how do we calculate the values of this output Matrix this is six by six this is the dot product of the first row with the First Column so this is a multiplied by a this the second value is the first row with the second column the third value is the first row with the third column until the last column so a multiplied by F Etc what is the dot product is basically you take the first number of the first row so here we have 512 numbers here we have 512 numbers so you take the first number of the first row and the first number of the First Column you multiply them together second value of the first row second value of the First Column you multiply them together and then you add all these numbers together so it will be let's say uh this number multiplied by this plus this number multiplied by this plus this number multiplied by this plus this number multiplied by this plus you sum all this number together and this is the a DOT product a so we should be familiar with this notation because I will be using it a lot in the next slides let's start our journey with of the Transformer uh by looking at the encoder so the encoder starts with the input embeddings so what is an input embedding first of all let's start with our sentence we have a sentence of in this case six words what we do is we tokenize it we transform the sentence into tokens what does it mean to tokenize we split them into single words it is not necessary to always split the sentence using single words we can even split the sentence in part in smaller parts that are even smaller than a single word so we could even split this a sentence into let's say 20 tokens by using the each by splitting each word into multiple words this is usually done in most modern Transformer models but we will not be doing it otherwise it's really difficult to visualize so let's suppose we have this input sentence and we split into tokens and each token is a single word the next step we do is we map these words into numbers and these numbers represent the position of these words in our vocabulary so imagine we have a vocabulary of all the possible words that appear in our training set each word will occupy a position in this vocabulary so for example the word will occupy the position 105 the word the cat will occupy the position 6500 Etc and as you can see this cat here has the same number as this cat here because they occupy the same position in the vocabulary we take these numbers which are called input IDs and we map them into a vector of size 512. this Vector is a vector made of 512 numbers and we always map the same word to always the same embedding however this number is not fixed it's a parameter for our model so our model will learn to change these numbers in such a way that it represents the meaning of the word so the input ID is never change because our vocabulary is fixed but the embedding will change along with the training process of the model so the embeddings numbers will change according to the needs of the loss function so the input embedding are basically mapping our single word into an embedding of size 512 and we call this quantity 512 D model because it's the same name that it's also used in the paper attention is all you need let's look at the next layer of the encoder which is the positional encoding so what is positional encoding what we want is that each word should carry some information about its position in the sentence because now we built a matrix of words that are embeddings but they don't convey any information about how where that particular word is inside the sentence and this is the job of the positional encoding so what we do we want the model to treat words that appear close to each other as close and words that are distant as distant so we want the model to see this information about the special information that we see with our eyes so for example when we see this sentence what is positional encoding we know that the word what is more far from the word um is compared to encoding because we we have this partial information given by our eyes but the model cannot see this so we need to give some information to the model about how the words are specially distributed inside of the sentence and we want the positional encoding to represent a pattern that the model can learn and we will see how imagine we have our original sentence your cat is a lovely cat what we do is we first convert into embeddings using the previous layer so the input embeddings and these are embeddings of size 512 then we create some special vectors called the positional encoding vectors that we add to these embeddings so this Vector we see here in red is a vector of size 512 which is not learned it's computed once and not learned along with the training process it's fixed and this word this Vector represents the position of the word inside of the sentence and this should give us a output that is a vector of size again 512 because we are summing this number with this number this number with this number so the First Dimension with the First Dimension the second dimension with that so we will get a new Vector of the same size of the input vectors or how are these position in both embedding calculated let's see imagine we have a smaller sentence let's say your cat is and you may have seen the following expressions from the paper what we do is we create a vector of five of size D model so 512 and for each position in this Vector we calculate the value using these two expressions using these arguments so the first argument indicates the position of the word inside of the sentence so the word your occupies the position zero and we use them for the even Dimension so the zero the two the four the 510 Etc we use the first expression so the sine and for the other positions of this Vector we use the second expression and we do this for all the words inside of the sentence so this particular embedding is calculated p e of 1 0 because it's the first word embedding zero so this one represents the argument pause and this 0 represents the argument 2 I and p e of 1 1 means that the first word uh Dimension one so we will use the cosine giving the position one and the two I will be equal to 2i plus 1 will be equal to 1. and we do this for this third word Etc if we have another sentence we will not have different positional encodings we will have the same vectors even for different sentences because the positional encoding are computed once and reused for every sentence that our model will see during inference or training so we only compute the positional encoding once when we create the model we save them and then we reuse them we don't need to compute it every time we feed the feed a sentence to the model so why the authors chose the cosine and the sine functions to represent positional encodings because let's watch the plot of these two functions uh the you can see the plot is by position so the position of the word inside of the sentence and this depth is the dimension along the vector so the two I that you see saw before in the previous expressions and if we plot them we can see as humans a pattern here and we hope that the model can also see this path okay the next layer of the encoder is the multi-head attention we will not go inside of the multi-head attention first we will first visualize the single head attention so the self-attention with a single head and let's do it so what is self-attention self attention is a mechanism that existed before they introduced the Transformer the Alters of the Transformer just changed it into a multi-head attention so how did the self-attention work the self-attention allows the model to relate words to each other okay so we had the input embeddings that capture the meaning of the word then we have the positional encoding that give the information about the position of the word inside of the sentence now we want this self-attention to relate words to each other now imagine we have uh in an input sequence of six word with the D model of size 512. which can be represented as a matrix that we will call Q K and V so our q k and V is a same Matrix are the same Matrix representing the input so the input of six words with the dimension of 512 so each word is represented by a vector of size 512. we basically apply this formula we saw here from the paper to calculate the attention the self attention in this case why self-attention because it's the each word in the sentence related to other words in the same sentence so it's self-attention so we start with our Q Matrix which is uh the input sentence so let's visualize it for example so we have six rows and on this uh on the columns we have 512 column now they are really difficult to draw but let's say we have 512 columns and here we have six okay now what we do according to this formula we multiply it by the same sentence but transposed so the transpose of the K which is again the same input sequence we divide it by the square root of 512 and then we apply this soft Max the output of this as we saw before in in the initial Matrix and notations we saw that when we multiply 6 by 512 with another Matrix that is 512 by 6 we obtain a new Matrix that is six by six and each value in this Matrix represents the dot product of the first row with the First Column this represents the dot product of the first row with the second column Etc the values here are actually randomly generated so don't concentrate on the values what you should notice is that the soft Max makes all these values in such a way that they sum up to one so this Row for example here some sums up to one this other row also sums up to one etc etc and this value we see here it's the dot product of the first word with the embedding of the word itself this value here is the dot product of the embedding of the word your with the embedding of the word cat and this value here is the dot product of the word the embedding of the word your with the embedding of the word is the next thing we and this value represents somehow a score that how intense is the relationship between one word and another let's go uh ahead with the formula so for now we just multiplied Q by K divided by the square root of Decay applied to the soft Max but we didn't multiply by V so let's go forward we multiply this matrix by V and we obtain a new Matrix which is 6 by 512 so if we multiply a matrix that is 6 by 6 with another that is 6 by 512 we get a new Matrix that is 6 by 512 and one thing you should notice is that with the dimension of this Matrix is exactly the dimension of the initial Matrix from which we started this what does it mean that we obtain a new Matrix that is six rows so let's say six rows with 512 columns in which each these are our words so we have six words and each word has an embedding of Dimension 512 so now this embedding here represents not only the meaning of the word which was given by the input embedding not only the position of the word which was added by the positional encoding but now somehow this special embedding so these values represent a special embedding that also captures the relationship of this particular word with all the other words and this particular embedding of this word here also captures not only its meaning not only its position inside of the sentence but also the relationship of this word with all the other words I want to remind you that this is not the multi-head attention we are just watching the self-attention so one head we will we will see later how this becomes the multi-head attention self-attention has some properties that are very desirable first of all it's permutation invariant what does it mean to be permutation invariant it means that if we have a matrix let's say first we had a matrix of six words in this case the let's say just four words so a b c and d and suppose by applying the formula before this produces this particular Matrix in which the there is new special embedding for the word a a new special embedding for the word b a new special bedding for the word c and d so let's call it a prime B Prime C Prime D Prime if we change the position of these two rows the values will not change the position of the output will change accordingly so the values of B Prime will not change it will just change in the the position and also the C will also change position but the values in each Vector will not change and this is a desirable properties self-attention as of now requires no parameters I mean I didn't introduce any parameter that is learned by the model I just took the initial sentence of in this case six words we multiplied it by itself we divide it by a fixed quantity which is the square root of 512 and then we apply the soft Max which is not introducing any parameters so for now the self-attention rate didn't require any parameter except for the embedding of the words this will change later when we introduce the multi-head attention also we expect because the each value in the self-attention in the soft Max Matrix is a DOT product of the word embedding with itself and the other words we expect the values along the diagonal to be the maximum because it's the dot product dot product of each word with itself and there is another property of this Matrix that is before we apply the soft softmax if we replace the value in this Matrix suppose we don't want the word your and Cat to interact with each other or we don't want the word let's say is and the lovely to interact with each other what we can do is before we apply the softmax we can replace this value with minus infinity and also this value with minus infinity and when we apply the soft Max the soft Max will replace minus infinity with 0. because as you remember the soft Max is e to the power of x if x is going to minus infinity e will be e to the power of minus infinity will become very very close to zero so basically zero this is a desirable property that we will use in the decoder of the Transformer now let's have a look at what is a multi-head attention so what we just saw was the self attention and we want to convert it into a multi-headed tension you may have seen these expressions from the paper but don't worry I will explain them one by one so let's go imagine we have our encoder so we are on the encoder side of of the Transformer and we have our input sentence which is let's say 6 by 512 so Six Word by 512 is the size of the embedding of each word in this case I call it sequence by D model so sequence is the sequence length as you can see on the legend in the bottom left of the slide and the D model is the size of the embedding Vector which is 512. what we do just like the picture shows and we take this input and we make four copies of it one will be sent uh wait one will be sent along this connection we can see here and three will be sent to the multi-header attention with three respective names so it's the same input that becomes three matrices that are equal to input one is called the query one is called key and one is called value so basically we are taking this input and making three copies of it one we call Q K and B they have of course the same dimension what does the multihead attention do first of all it multiplies these three matrices by three parameter matrices called WQ w k and WV these matrices have Dimension D model by D model so if we multiply a matrix that is sequence by the model with another one that is D model by D model we get a new Matrix as output that is sequenced by D model so basically the same Dimension as the starting Matrix and we will call them Q Prime K Prime and V Prime our next step is to split these matrices into smaller matrices let's see how we can split this Matrix Q Prime by the sequence Dimension or by the D model dimension in the multi-hat attention we always split by the D model Dimension so every head will see the full sentence but a smaller part of the embedding of each word so if we have an embedding of let's say 512 it will become smaller embeddings of 512 divided by four and we call this quantity d k so d k is D model divided by H where H is the number of heads in our case we have H equal to 4. we can calculate the attention between these smaller matrices so q1 K1 and V1 using the expression taken from the paper and this will result into a small Matrix called Head 1 head 2 head 3 and head four the dimension of head 1 up to head four is sequence by d v what is DV is basically it's equal to DK it's just called a DV because the last multiplication is done by V and in the paper they call it DV so I am also sticking to the same names our next step is to multi combine these matrices these small heads by concatenating them along the DV Dimension just like the paper says so we can cut all this head together and we get a new Matrix that is sequence by H multiplied by DV where H multiplied by DV as we know DV is equal to d k so H multiplied by DV is equal to D model so we get back the initial shape so it's sequence by D model here the next step is to multiply the result of this concatenation by w o and W O is a matrix that is H multiplied by DV so D model multiple with the other dimension being T model and the result of this is a new Matrix that is the result of the multi-head attention which is sequenced by D model so the multi had attention instead of calculating the attention between these matrices here so Q Prime K Prime and V Prime splits them along the D model Dimension into smaller matrices and calculates the attention between these smaller matrices so each head is watching the full sentence but as different aspect of the embedding of each word why we want this because we want the each head to watch different aspects of the same word for example in the Chinese language but also in other languages one word may be a noun in some cases maybe a verb in some other cases maybe a adverb in some other cases depending on the context so what we want is that one head maybe learns to relate that word as a noun another head maybe learns to relate that word as a verb and another head learn to release that verb as an objective or adverb so this is why we want a multi-head attention now you may also have seen online that the the attention can be visualized and I will show you how when we calculate the attention between the Q and the K matrices so when we do this operation so the soft Max of Q multiplied by the K divided by the square root of d k we get a new Matrix just like we saw before which is sequenced by sequence and this represents a score that represents the intensity of the relationship between the two words we can visualize this and this will produce a visualization uh similar to this one which I took from the paper in which we see how the all the heads work so for example if we concentrate on this work making this word here we can see that making is related to the word difficult so this word here by different heads so the blue head the red head and the green head but the wire let's say the Violet head is not relating this two word together so making and difficult is not related by the violet or the pink head The Violet head or the pink head they are relating the word making to other words for example to this word 2009 why this is the case because maybe this pink head could see the part of the embedding that these other heads could not see that made this interaction possible between these two words you may be also wondering why these three mattresses are called query keys and values okay the terms come from the database terminology or from the python-like dictionaries but I would also like to give my interpretation of my own making a very simple example I think it's quite easy to um understand so imagine we have a python-like dictionary or a database in which we have keys and values the keys are the category of movies and the values are the movies belonging to that category in my case I just put one value so we have Romantics category which includes Titanic we have action movies that include the Dark Knight Etc imagine we also have a user that makes a query and the query is love because we are in the Transformer world all these words actually are represented by embeddings of size 512. so what our Transformer will do he will convert this word love into an embedding of 512 all these queries and values are already embeddings of 512 and it will calculate the dot product between the query and all the keys just like the formula so as you remember the formula is a soft Max of query multiplied by the transpose of the keys divided by the square root of the model so we are doing the dot product of all the queries with all the keys in this case the word love with all the keys one by one and this will result in a score that will amplify some values or not amplify other values um in this case our embedding may be in such a way that the word love and romantic are inter are related to each other the word love and comedy are also related to each other but not so intensively like the word love and romantic so it's more how to say let's less strong relationship but maybe the word horror and love are not related at all so maybe their soft Max score is very close to zero our next um layer in the encoder is the ADD and norm and to introduce the other Norm we need the layer normalization so let's see what is the layer normalization layer normalization is a layer that okay let's make a practical example imagine we have a batch of n items in this case n is equal to three item one item two item three each of these items will have some features it could be an embedding so for example it could be a feature of a vector of size 512 but it could be a very big Matrix of thousands of features doesn't matter what we do is we calculate the mean and the variance of each of these items independently from each other and we replace each value with another value that is given by this expression so basically we are normalizing so that the new values are all in the range 0 to 1. actually we also multiply this new value with a parameter called gamma and then we add another parameter called beta and this gamma and beta are learnable parameters and the model should learn to multiply and add these parameters so as to amplify the value that it wants to be Amplified and not amplify that value that it doesn't want to be Amplified uh so we don't just normalize we actually introduce some parameters and I found a really nice visualization from papers with code.com in which we see the difference between batch norm and layer Norm so as we can see in the layer normalization we are calculating if n is the batch Dimension we are calculating all the values belonging to one item in the batch while in the batch Norm we are calculating the same feature for all the batch so for all the items in the batch so we are mixing let's say values from different items of the batch while in the layer normalization we are treating each item in the batch independently which will have its own mean and its own variance let's look at the decoder now um in the encoder we saw the input embeddings in this call in this case they are called output embeddings but the underlying working is the same here also we have the positional encoding and they are also the same as the Imp as the encoder the next layer is the musket multi-head attention and we will see it now we also have the multi-head attention here with the here we should see that the there is the encoder here that produces the output and is sent to the decoder in the forms of keys and values while the query so this connection here is the query coming from the decoder so in this multi-head attention it's not a self-attention anymore it's a cross attention because we are taking two sentences one is sent from the encoder side so let's write encoder in which we provide the output of the encoder and we use it as a query as keys and values while the output of the masked multi-head attention is used as the query in this multi-head attention and the musket multi-head attention is the self-attention of the input sentence of the decoder so we take the input sentence of the decoder we transform into embeddings we add the depositional encoding we give it to this multi-head attention in which the query key and values are the same input sequence we do the ADD and Norm then we send this as the queries of the multi-head attention while the keys and the values are coming from the encoder then we do the add the norm I will not be showing the feed forward which is just a fully connected layer we then send the output of the feed forward to the ADD and norm and finally to the linear layer which we will see later so let's have a look at the Muscat multi-head attention and how it differs from a normal multi-head attention what we want our goal is that we want to make the model causal it means that the output at a certain position can only depend on the words on the previous position so the model must not be able to see future words how can we achieve that as you saw the the output of the soft Max in the attention calculation formula is this Matrix sequence by sequence if we want to hide the interaction of some words with other words we delete this value and we replace it with minus infinity before we apply the soft Max so that the soft Max will replace this value with 0. and we do this for all the interaction that we don't want so we don't want your to watch future words so we don't want your to watch cat is a lovely cat and we don't want the word cat to watch future words but only all the words that come before it or the word itself so we don't want this this this this also the same for the other words Etc so we can see that we are replacing all the word all this values here that are above this diagonal here so this is the principal diagonal of the Matrix and we want all the values that are above this diagonal to be replaced with minus infinity so that so that the soft Max will replace them with zero let's see in which stage of the multi-head attention this mechanism is introduced so when we calculate the attention between these smaller matrices so q1 K1 and V1 before we apply this soft Max we replace this values so this one this one this one this one this one Etc with minus infinity then we apply this soft Max and then the soft Max will take care of transforming these values into zeros so basically we don't want these words to interact with each other and if we don't want this interaction the model will learn to not make them interact because the model will not get any information from this interaction so it's like this word cannot interact now let's look at how the inference and training works for a Transformer model as I saw said previously we are dealing with it we will be dealing with the translation tasks so because it's easy to visualize and it's easy to understand all the steps let's start with the training of the model we will go from an English sentence I love you very much into an Italian sentence it's a very simple sentence it's easy to describe let's go we start with a description of the of the Transformer model and we start with our English sentence which is sent to the encoder so our English sentence here on which we prepared and append to special tokens one is called start of sentence and one is called end of sentence these two tokens are taken from the vocabulary so they are special tokens in our vocabulary that tells the model what is the start position of a sentence and what is the end of a sentence we will see later why we need them for now just think that we take our sentence we prepend a special token and we append a special token then what we do as you can see from the picture we take our inputs we transform into input embeddings we add the positional encoding and then we send it to the encoder so this is our encoder input sequence by the model we send it to the encoder it will produce an output which is encode a sequence by D model and it's called the encoder output so as I saw we saw previously the output of the encoder is another Matrix that has the same Dimension as the input Matrix in which the embedding we can see it as a sequence of embeddings in which this embedding is special because it captures not only the meaning of the word which was given by the input embedding we saw here so by this not only the position which was given by the positional encoding but also the interaction of every word with every other word in the same sentence because this is the encoder so we are talking about self-attention so it's the interaction of each word in the sentence with all the other words in the same sentence we want to convert this sentence into Italian so we prepare the input of the decoder which is a start of sentence as you can see from the picture of the the Transformer the outputs here you can see shifted right what does it mean to shift right basically it means we prepared a special token called SOS start of sentence you should also notice that these two sequences actually they in when we code the Transformer so if you watch my other video on how to code a Transformer you will see that we make this sequence of fixed length so that if we have a sentence that is te amo multo or a very long sequence actually when we feed them to the Transformer they all becomes become of the same length how to do this we add padding words to reach the length the desired length so if our model can support let's say a sequence length of 1000 in this case we have a fourth tokens we will add 996 tokens of padding to make this sentence long enough to reach the sequence length of course I'm not doing it here because it's not easy to visualize otherwise okay we prepared this input for the decoder we add transform into embeddings we add the positional encoding then we send it first to the multi-head attentions to the musket multi-haditation so along with the causal mask and then we take the output of the encoder and we send it to the decoder as keys and values while the queries are coming from the musket so the queries are coming from this layer and the keys and the values are the output of the encoder this the output of all this block here so all this big block here will be a matrix that is sequence by the model just like for the encoder however we can see that this is still an embedding because it's a D model it's a vector of size 512 how can we relate this um embedding back into our dictionary how can we understand what is this word in our vocabulary that's why we need a linear layer that will map sequence by D model into another sequence by vocabulary size so it will tell for every embedding that it sees what is the position of that word in our vocabulary so that we can understand what is the actual token that is output by the model after that we apply the softmax and then we have our label what we expect the model to Output given this English sentence we expect the model to Output this te amo multo end of sentence and this is called the label or the target what we do when we have the output of the model and the corresponding label we calculate the loss in this case is the cross entropy loss and then we back propagate the loss to all the weights now let's understand why we have these special tokens called SOS and EOS basically you can see that here the sequence length is 4 actually is 1000 because we have the padding but let's say we don't have any padding so it's four tokens start of sentence the ammo multo and what we want is the T ammo multo end of sentence so our model when it will see the start of sentence token it will output the first token as output T when it will see T it will output ammo when it will see armor it will output molto and when it will see a multo it will output end of sentence which will indicate that okay the translation is done and we will see this mechanism in the inference ah this all happens in one time step just like I promised at the beginning of the video I said that with recurrental or neural networks we have end time steps to map n input sequence into an output sequence but this problem would be solved with the Transformer yes it has been solved because you can see here we didn't do any for Loop we just did all in one pass we give an input sequence to the encoder an input sequence to the decoder we produced some outputs we calculated that cross entropy loss with the label and that's it it all happens in one time step and this is the power of the Transformer because it made it very easy and very fast to train very long sequences and with the very very nice performance that you can see in charge GPD you can see GPT in bird Etc let's have a look at how inference works again we have our English sentence I love you very much we want to map it into an Italian sentence we have our usual Transformer we prepare the input for the encoder which is start of sentence I love you very much end of sentence we convert into input embeddings then we add the positional encoding we prepare the input for the encoder and we send it to the encoder the encoder will produce an output which is sequenced by the model and we saw it before that it's a sequence of special embeddings that capture the meaning the position but also the interaction of all the words with other words what we do is for the decoder we give him just the start of sentence and of course we keep the we add enough embedding padding tokens to reach our sequence length we just give the model the start of sentence token and we again we for this single token we convert into embeddings we add the positional encoding and we send it to the decoder as decoder input the decoder will take this um his input as a query and the key and the values coming from the encoder and it will produce an output which is sequenced by D model again we want the linear layer to project it back to our vocabulary and this projection is called logits what we do is we apply the soft Max which will select given the logists will give the position of the output word will have the maximum score with the soft Max this is how we know what words to select from the vocabulary and this hopefully should produce the first output token which is T if the model has been trained correctly this however happens at time step one so when we train the model Transformer model it happens in one pass so we have one input sequence one output sequence we give it to the model we do it one time step and the model will learn it when we inference however we need to do it token by token and we will also see why this is the case at time Step 2 we don't need to recompute the encoder output again because the over English sentence didn't change so we hope the the encoder should produce the same output for it and then what we do is we take the output of the previous sentence so um as T we append it to the input of the decoder and then we feed it to the decoder again with the output of the encoder from the previous step which will produce an output sequence from the decoder side which we again project back into our vocabulary and we get the next token which is ammo so as I saw before as I as I said before we are not recalculating the output of the encoder for every time step because our English sentence didn't change at all what is changing is the input of the decoder because at every time step we are appending the output of the previous step to the input of the decoder we do the same for the time step 3 and we do the same for the time step 4 and hopefully we will stop when we see the end of sentence token because that is that's how the model tells us to stop inferencing and this is how the inference works why we needed four time steps when we inference a model um like the in this case the translation model there are many strategies for inferencing what we used is called greedy strategy so for every step we get the word with the maximum soft max value and however this strategy Works uh usually not bad but there are better strategies and one of them is called beam search in beam search instead of always greedily so this is that's why it's called greedy instead of greedily taking the maximum soft value we take the top B values and then for each of these choices we inference what are the next possible tokens for each of the top B values at every step and we keep only the one with the B most probable sequences and we delete the others this is called beam search and it generally it performs better so thank you guys for watching uh I know it was a long video but it was really worth it to go through each aspect of the Transformer I hope you enjoyed this journey with me so please subscribe to the channel and don't forget to watch my other video on how to code a Transformer model from scratch in which I describe not only again the structure of the Transformer model while coding it but I also show you how to train it on a data set of your choice how to inference it and I also provided the code on GitHub and the Ecolab notebook to train the model directly on collab please subscribe to the to the channel and let me know what you didn't understand so that I can give more explanation and please tell me what are the problems in this kind of videos or in this particular video that I can improve for the next videos thank you very much and have a great rest of the day\")]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NG9nTFyUOwCm"
      },
      "source": [
        "### Split Content into Chunks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "i8sC_0i6Oxw9"
      },
      "outputs": [],
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
        "splits = text_splitter.split_documents(video_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kqo8QfvNPKqO"
      },
      "source": [
        "### Initialize Embeddings and Chroma Vector Store"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "uCersyHBPLkQ"
      },
      "outputs": [],
      "source": [
        "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
        "\n",
        "# Create a Chroma vector store\n",
        "vectorstore = Chroma.from_documents(splits, embeddings)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sVlNUtzfPRDx"
      },
      "source": [
        "### Set Up Conversational Retrieval Chain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "-x2tV1EaPR6Q"
      },
      "outputs": [],
      "source": [
        "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0.7)\n",
        "\n",
        "qa = ConversationalRetrievalChain.from_llm(\n",
        "    llm=llm,\n",
        "    retriever=vectorstore.as_retriever(),\n",
        "    memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
        "  )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oLf97WPqPbQB"
      },
      "source": [
        "### Chat Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "DQOmqnYMPcMy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "86bc0d2a-45a2-437b-f3b9-c42c02e717ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Human: What is the main topic of this video?\n",
            "AI: The main topic of the video is about the Transformer model, specifically a revised version (2.0) of a previous series on the Transformer. The video aims to improve audio quality and discusses the structure of the Transformer model, how to code it from scratch, train it on a dataset, and perform inference.\n"
          ]
        }
      ],
      "source": [
        "def chat_with_video(query):\n",
        "    result = qa.invoke({\"question\": query})\n",
        "    return result['answer']\n",
        "\n",
        "# Example usage\n",
        "query = \"What is the main topic of this video?\"\n",
        "response = chat_with_video(query)\n",
        "print(f\"Human: {query}\")\n",
        "print(f\"AI: {response}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E4TFZbg-P6Nm"
      },
      "source": [
        "###Create an Interactive Chat Interface"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "F9AvilyiP7Hm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175,
          "referenced_widgets": [
            "09935ab37db44722a358938187cca50b",
            "416cef777bf749e0a81572b4850ab056",
            "8f82f9debb894177a3bfe6fd73222dff",
            "cce372f60ada4d27998397c893ef276c",
            "3b59e3bd9ad74089bc1d874ddb366a72",
            "0350a38331de4fa7b7c38f5b06e6f2bd",
            "4dc88579e1a845188933f055f29d3d10",
            "79e551032da6480e8dd0d03a410cf423",
            "a3751e733c9a406b93a953e44d3aaf12",
            "ef01e13080694be1810449b891462f6a"
          ]
        },
        "outputId": "c323a173-e1d8-4387-8233-0d31181159c8"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<h3>Chat with Video</h3>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Text(value='', description='You:'), Button(description='Send', style=ButtonStyle()), Output()))"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "09935ab37db44722a358938187cca50b"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "from IPython.display import display, HTML\n",
        "from ipywidgets import widgets\n",
        "\n",
        "chat_history = []\n",
        "\n",
        "def on_send_button_clicked(b):\n",
        "    query = input_box.value\n",
        "    input_box.value = \"\"\n",
        "\n",
        "    response = chat_with_video(query)\n",
        "\n",
        "    chat_history.append(f\"Human: {query}\")\n",
        "    chat_history.append(f\"AI: {response}\")\n",
        "\n",
        "    output.clear_output()\n",
        "    with output:\n",
        "        print(\"\\n\".join(chat_history))\n",
        "\n",
        "input_box = widgets.Text(description=\"You:\")\n",
        "send_button = widgets.Button(description=\"Send\")\n",
        "output = widgets.Output()\n",
        "\n",
        "send_button.on_click(on_send_button_clicked)\n",
        "\n",
        "display(HTML(\"<h3>Chat with Video</h3>\"))\n",
        "display(widgets.VBox([input_box, send_button, output]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8G8X0gKhQOOP"
      },
      "source": [
        "### Classwork 1\n",
        "\n",
        "1. Try different open source models using Together\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MejUuzQyQaUK"
      },
      "source": [
        "### Classwork 2\n",
        "\n",
        "1. Create a bot for a famous personality (Bill Gates, Mahatma Gandhi, etc) - add system instructions + image\n",
        "2. Create a bot for a use-case/scenario (Interview prep, a chatbot for a specific service,  etc )\n",
        "\n",
        "1. Create a QA engine on CSV/Audio/Video/PDF\n",
        "2. Experiment with different chunks, models, vector dbs."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "09935ab37db44722a358938187cca50b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_416cef777bf749e0a81572b4850ab056",
              "IPY_MODEL_8f82f9debb894177a3bfe6fd73222dff",
              "IPY_MODEL_cce372f60ada4d27998397c893ef276c"
            ],
            "layout": "IPY_MODEL_3b59e3bd9ad74089bc1d874ddb366a72"
          }
        },
        "416cef777bf749e0a81572b4850ab056": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "TextModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "TextModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "TextView",
            "continuous_update": true,
            "description": "You:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_0350a38331de4fa7b7c38f5b06e6f2bd",
            "placeholder": "​",
            "style": "IPY_MODEL_4dc88579e1a845188933f055f29d3d10",
            "value": ""
          }
        },
        "8f82f9debb894177a3bfe6fd73222dff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Send",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_79e551032da6480e8dd0d03a410cf423",
            "style": "IPY_MODEL_a3751e733c9a406b93a953e44d3aaf12",
            "tooltip": ""
          }
        },
        "cce372f60ada4d27998397c893ef276c": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_ef01e13080694be1810449b891462f6a",
            "msg_id": "",
            "outputs": [
              {
                "output_type": "stream",
                "name": "stdout",
                "text": [
                  "Human: https://www.youtube.com/watch?v=bCz4OMemCcA\n",
                  "AI: I'm sorry, but I cannot access or view external links, including YouTube videos. Therefore, I am unable to determine the main topic of the video at the provided link.\n"
                ]
              }
            ]
          }
        },
        "3b59e3bd9ad74089bc1d874ddb366a72": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0350a38331de4fa7b7c38f5b06e6f2bd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4dc88579e1a845188933f055f29d3d10": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "79e551032da6480e8dd0d03a410cf423": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a3751e733c9a406b93a953e44d3aaf12": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "ef01e13080694be1810449b891462f6a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}