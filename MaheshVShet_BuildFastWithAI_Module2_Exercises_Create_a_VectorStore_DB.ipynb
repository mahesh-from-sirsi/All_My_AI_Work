{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mahesh-from-sirsi/All_My_AI_Work/blob/main/MaheshVShet_BuildFastWithAI_Module2_Exercises_Create_a_VectorStore_DB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 1: Create a VectorStore DB\n",
        "Load a document (PDF or TXT), process its text, apply embeddings, and store the results in ChromaDB OR FAISS for efficient retrieval.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "lJK5OaVgHcdW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## STEPS IN STORING THE DATA TO THE VECTOR STORE FOR RAG:\n",
        "\n",
        "### Step1: Import all needed libraries\n",
        "\n",
        "### Step2: Define all the necessary methods\n",
        "\n",
        "### Step3: Initialization and model definition\n",
        "\n",
        "### Step4: Load the text that you want to chunk\n",
        "\n",
        "### Step3: Convert the text into multiple chunks\n",
        "\n",
        "### Step4: Intialize the embedding model\n",
        "\n",
        "### Step5: create vectors of all the chunks\n",
        "\n",
        "### Step6: Initialize the Vector Database\n",
        "\n",
        "### Step7: Store the Data in the Vector Database"
      ],
      "metadata": {
        "id": "6CIHc7YdpjOl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Hint:  \n"
      ],
      "metadata": {
        "id": "ERqkAHIyf0c-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU langchain langchain-openai langchain-pinecone langchain_community pypdf chromadb"
      ],
      "metadata": {
        "id": "vPtDZ-Hdz_e9"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 1: IMPORT ALL THE REQUIRED MODULES\n",
        "\n",
        "# Import all the necessary modules\n",
        "import os                                                          # To Save the secrets as environment\n",
        "import numpy as np                                                 # For cosine similarity\n",
        "from langchain_openai import OpenAIEmbeddings                      # library for creating the embedding model\n",
        "from langchain_community.document_loaders import PyPDFLoader       # Library for reading the PDF as a PDFLoader\n",
        "#from langchain.text_splitter import CharacterTextSplitter         # Library for splitting the text into chunks\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter # Library for splitting the text into chunks\n",
        "from langchain_community.vectorstores import Chroma                # Library for interacting with the ChromaDB vector store\n",
        "from langchain.chains import RetrievalQA                           # For Converting UserQuery to Embeddings and retrive matching entry from top k entries\n",
        "from langchain.chat_models import ChatOpenAI                       # or any other LLM\n"
      ],
      "metadata": {
        "id": "80flwj5j0Dfu"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP2 - DEFINE ALL NECESSARY METHODS\n",
        "## Function for converting text to embedding\n",
        "def embed_text(text):\n",
        "    text_vector = embed_model.embed_query(text)\n",
        "    return text_vector\n",
        "\n",
        "\n",
        "# FORMULAE FOR COSINE SIMILARITY IS\n",
        "# cosine_similarity = np.dot(A, B) / (norm(A) * norm(B))\n",
        "#\n",
        "# This will output a value between -1 and 1 indicating how similar the two\n",
        "# vectors are in direction (1 means exactly the same, 0 means orthogonal,\n",
        "# -1 means opposite directions).\n",
        "\n",
        "def cosine_similarity(statement1, statement2):\n",
        "    vec1 = embed_text(statement1)\n",
        "    vec2 = embed_text(statement2)\n",
        "    dot_product = np.dot(vec1, vec2)\n",
        "    norm_vec1 = np.linalg.norm(vec1)\n",
        "    norm_vec2 = np.linalg.norm(vec2)\n",
        "    similarity = dot_product / (norm_vec1 * norm_vec2)\n",
        "    return similarity"
      ],
      "metadata": {
        "id": "qQ5pi3J3rmHs"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP3 - ALL INITIALIZATION, MODEL CREATION\n",
        "\n",
        "# Save the Key needed for interacting with the Model, Database etc,\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
        "\n",
        "# Create the embedding model\n",
        "embed_model = OpenAIEmbeddings(model = \"text-embedding-3-small\")\n",
        "\n",
        "# Step 2: Split text into chunks for better processing\n",
        "# text_splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=50)  # Define chunk size and overlap\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)"
      ],
      "metadata": {
        "id": "Kf_5mJLb2ljM"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP4 - Read the text you want to store in the Vector Store (Load a document (TXT or PDF))\n",
        "loader = PyPDFLoader(\"/content/MaheshVShet-CV.pdf\")  # Use PyPDFLoader for PDFs\n",
        "\n",
        "# Read the pages of the PDF file\n",
        "documents = loader.load() # documents is now a list of LangChain Document objects, each containing page_content + metadata (like page number).\n",
        "\n",
        "# Text Splitter\n",
        "docs = text_splitter.split_documents(documents)"
      ],
      "metadata": {
        "id": "aBQmSP-h2qdQ"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the Vector Store\n",
        "vectorstore = Chroma.from_documents(docs, embed_model, persist_directory=\"chroma_db\")\n",
        "\n",
        "# Persist the database\n",
        "vectorstore.persist()"
      ],
      "metadata": {
        "id": "t1He9DxG3JXy"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Search the contents of the vector store\n",
        "\n",
        "# RetrievalQA is a LangChain abstraction that ties together a retriever\n",
        "# (like ChromaDB) with a language model (LLM) so you can ask natural questions\n",
        "# over your documents.\n",
        "\n",
        "# Without RetrievalQA\n",
        "# You would have to:\n",
        "# - Take the user’s query → embed it.\n",
        "# - Search the vector DB (e.g., Chroma) → get the top-k chunks.\n",
        "# - Concatenate those chunks into a context.\n",
        "# - Pass that context + the query into the LLM manually.\n",
        "# - Parse and return the response.\n",
        "# - This means you’re handling retrieval + formatting + QA pipeline logic yourself.\n",
        "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm=ChatOpenAI(model=\"gpt-3.5-turbo\"),\n",
        "    retriever=retriever,\n",
        ")\n",
        "\n",
        "query = \"Summarize the PDF in 3 bullet points.\"\n",
        "print(qa_chain.run(query))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RNFiFj0E6PzH",
        "outputId": "3eb32982-381e-4f85-c853-6e850803fb67"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Successful management of Development and QE teams to ensure collaboration and continuous improvement culture\n",
            "- Ensured 100% on-time delivery of endpoint security solutions without compromising quality or security standards\n",
            "- Published research paper on \"Usage of Jumbo Frames in Virtualised Environments,\" filed a patent for \"Configuring a Virtual Machine Remotely\" technology, and developed solutions improving virtual machine performance by 25% in enterprise environments\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"How old is Mahesh V. Shet\"\n",
        "print(qa_chain.run(query))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HtiJIf3u1EVH",
        "outputId": "2f4a24e9-4e1d-43fe-fbc0-dc7a4c94fb5f"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mahesh V. Shet has over 24 years of experience, and his role as Vice President of the Staff Welfare Association at ISRO was from 1999 to 2003. Based on this information, it can be inferred that Mahesh V. Shet is likely in his mid-40s to early 50s.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 2 : Response to User Query\n",
        "After storing a document in a vector database (e.g., ChromaDB or FAISS), write a function that takes a user's query as input and returns a relevant response based on the stored content.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "NWPUZkTrIg2B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Hint:  \n"
      ],
      "metadata": {
        "id": "poGzPpOafSmi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Define a function to query the database\n",
        "def get_response(user_query, vector_db, k=3):\n",
        "    # Search for similar documents/chunks\n",
        "    relevant_docs = vector_db.similarity_search(user_query, k=k)\n",
        "\n",
        "    # Extract and return the content of the relevant documents\n",
        "    return [doc.page_content for doc in relevant_docs]\n",
        "\n",
        "# Step 3: Test the function with a user query"
      ],
      "metadata": {
        "id": "fHb5GxNSIg7q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fKWwnoVHkECe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}