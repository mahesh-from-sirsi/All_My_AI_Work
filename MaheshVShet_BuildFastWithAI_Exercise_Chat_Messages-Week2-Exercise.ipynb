{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mahesh-from-sirsi/All_My_AI_Work/blob/main/MaheshVShet_BuildFastWithAI_Exercise_Chat_Messages-Week2-Exercise.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 1 :  Chat Messages\n",
        "Use HumanMessage and AIMessage from LangChain to interact with a Large Language Model (LLM). Ask the LLM a question about Generative AI and print its response.\n"
      ],
      "metadata": {
        "id": "LwzyXAIxK40G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Hint\n"
      ],
      "metadata": {
        "id": "PUEDlQytKm6E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU langchain==0.3.4 langchain-openai==0.2.12 langchain-together==0.2.0"
      ],
      "metadata": {
        "id": "Ced0uV_2xMlh"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.environ['OPENAI_API_KEY'] = \"\""
      ],
      "metadata": {
        "id": "R4yRepeuxQ2T"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Using OpenAI Models\n",
        "from langchain_openai import ChatOpenAI\n",
        "gpt4o_mini_model = ChatOpenAI(model = \"gpt-4o-mini\")\n",
        "\n"
      ],
      "metadata": {
        "id": "Y_1TKJvAxfSP"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage\n",
        "\n",
        "# Do neccessary imports and call llm\n",
        "messages = [SystemMessage(content=\"You are an AI expert. Explain concepts in a clear and detailed manner.\")]\n",
        "messages.append(HumanMessage(content=\"What is Generative AI?\"))\n",
        "#Give messages to llm and invoke\n",
        "response = gpt4o_mini_model.invoke(messages)"
      ],
      "metadata": {
        "id": "vvVqy5SUBm2Y"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(response.content)"
      ],
      "metadata": {
        "id": "ghhGBJY4PFEM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "428c5914-8d10-4ae9-c817-bc7043d0b34c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generative AI refers to a class of artificial intelligence techniques that involve creating new content, such as images, text, audio, or other forms of data, based on learned patterns from existing data. Unlike traditional AI, which typically focuses on analyzing and making predictions from existing data, generative AI actively generates new instances that resemble the training data.\n",
            "\n",
            "### Key Concepts of Generative AI:\n",
            "\n",
            "1. **Types of Models**:\n",
            "   - **Generative Adversarial Networks (GANs)**: This involves two neural networks, a generator and a discriminator, that work against each other. The generator creates new data instances, while the discriminator evaluates them against real data, providing feedback to improve the generator's outputs.\n",
            "   - **Variational Autoencoders (VAEs)**: These models learn to encode input data into a compressed representation and then decode that representation back into data. VAEs are particularly useful for generating new samples that are similar to the training data.\n",
            "   - **Transformers**: Models like OpenAI’s GPT (Generative Pre-trained Transformer) are based on transformer architecture and are particularly powerful for generating text. They learn to predict the next word in a sentence given the preceding words, allowing for coherent and contextually relevant text generation.\n",
            "\n",
            "2. **Applications**:\n",
            "   - **Text Generation**: Generative AI can produce written content, such as articles, stories, and dialogues. It’s used in chatbots and virtual assistants.\n",
            "   - **Image Generation**: GANs and other models can create realistic images from scratch, making them useful in fields like art, design, and gaming.\n",
            "   - **Music and Audio Generation**: AI can compose music or generate sound effects, providing new tools for musicians and audio engineers.\n",
            "   - **Video and Animation**: Generative models can also create or manipulate video content, although this is more complex and still an area of active research.\n",
            "\n",
            "3. **Training Process**:\n",
            "   - Generative AI models are trained on large datasets. The quality and diversity of the training data significantly influence the model's ability to generate high-quality outputs.\n",
            "   - The training involves adjusting model parameters to minimize the difference between the generated outputs and the actual data from the training set, often using techniques like backpropagation.\n",
            "\n",
            "4. **Challenges**:\n",
            "   - **Quality Control**: Ensuring that the generated content is high-quality, relevant, and coherent can be challenging.\n",
            "   - **Ethical Concerns**: Generative AI raises issues related to copyright, authenticity, and the potential for misuse (e.g., creating fake news, deepfakes).\n",
            "   - **Bias**: If the training data contains biases, the generated outputs may reflect or amplify those biases.\n",
            "\n",
            "5. **Future Directions**:\n",
            "   - As research continues, generative AI is expected to become more advanced, producing even more realistic and contextually aware outputs. There is also a growing focus on making models more efficient and accessible, as well as addressing ethical considerations and biases in generated content.\n",
            "\n",
            "In summary, generative AI represents a powerful and rapidly evolving area of artificial intelligence that enables the creation of new content based on learned patterns, with a wide range of applications and significant implications for various industries.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Exercise 2 : Multi-Turn Conversation\n",
        "\n",
        "Start with a `SystemMessage` defining the LLM's role as a tutor. The user asks a question, and the LLM responds. Then, the user asks a follow-up question, and the LLM provides a further explanation, all while maintaining its role as a tutor.\n",
        "\n"
      ],
      "metadata": {
        "id": "XTZI666-Ah-Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Hint\n"
      ],
      "metadata": {
        "id": "0S3ioDkAKrki"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from langchain.schema import SystemMessage, HumanMessage, AIMessage - This is the old way of importing the Messages.\n",
        "# starting 0.1.x version these classes are available in langchain_core.messages module and old will be eventually\n",
        "# deprecated\n",
        "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage\n",
        "\n",
        "# System message defining the LLM's role\n",
        "messages = [SystemMessage(content=\"You are a knowledgeable tutor. Your goal is to explain complex topics in a simple and engaging way.\")]\n",
        "messages.append(HumanMessage(content=\"What is backpropagation in neural networks?\"))\n",
        "# now ask questions from llm and append few more human and ai messages"
      ],
      "metadata": {
        "id": "aL8laFjJAiGL"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = gpt4o_mini_model.invoke(messages)"
      ],
      "metadata": {
        "id": "1LOfam_5PFpq"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CkIXUodQzzH6",
        "outputId": "179b18c9-99d3-4632-a301-b1e428f3ec08"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Backpropagation is a fundamental concept in training neural networks. To understand it, let’s break it down step-by-step.\n",
            "\n",
            "### What is Backpropagation?\n",
            "\n",
            "Backpropagation, short for \"backward propagation of errors,\" is an algorithm used to train neural networks. It helps the network learn from its mistakes by adjusting the weights of the connections between neurons based on the errors it makes in predictions.\n",
            "\n",
            "### How Does It Work?\n",
            "\n",
            "1. **Forward Pass**: \n",
            "   - When you input data into the neural network, it goes through layers of neurons, each performing calculations based on the current weights. This process is known as the forward pass.\n",
            "   - The output of the network is compared to the actual target output (the correct answer) to determine how far off the prediction is. This difference is called the error or loss.\n",
            "\n",
            "2. **Calculating the Error**:\n",
            "   - The network uses a loss function to quantify the error. Common loss functions include Mean Squared Error for regression tasks or Cross-Entropy Loss for classification tasks. The loss function gives a single number that represents how well the network is performing.\n",
            "\n",
            "3. **Backward Pass**:\n",
            "   - Here’s where the magic of backpropagation happens. The algorithm calculates how much each weight contributed to the error. This is done using the chain rule from calculus, which allows us to compute gradients.\n",
            "   - For each weight in the network, backpropagation computes the gradient of the loss function with respect to that weight. This tells us how much we need to adjust the weight to minimize the loss.\n",
            "\n",
            "4. **Weight Update**:\n",
            "   - Once we have the gradients, we can update the weights using an optimization algorithm like Stochastic Gradient Descent (SGD). The weights are adjusted in the opposite direction of the gradient to reduce the error.\n",
            "   - This process is repeated for many iterations (or epochs) over the training dataset.\n",
            "\n",
            "### Why is Backpropagation Important?\n",
            "\n",
            "- **Efficiency**: Backpropagation allows us to efficiently compute gradients for all weights in the network, even for deep networks with many layers.\n",
            "- **Learning**: It’s the backbone of how neural networks learn. Without backpropagation, training a neural network would be much more complicated and time-consuming.\n",
            "\n",
            "### In Summary\n",
            "\n",
            "Backpropagation is like teaching a neural network by showing it where it went wrong. It calculates the error, finds out how much each weight contributed to that error, and adjusts the weights to improve future predictions. This iterative process is what enables neural networks to learn patterns from data over time.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "human_message = HumanMessage(content= \"What is the cut-off day mean in case of LLM's\")\n",
        "messages[1] = human_message\n",
        "response = gpt4o_mini_model.invoke(messages)\n",
        "print(response.content)\n",
        "# print(messages[1].content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pokmlV9yz7F8",
        "outputId": "9685e0f1-7972-4576-96a7-9c70f71f2445"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In the context of Legal Language Models (LLMs), the term \"cut-off day\" refers to the specific date up to which the model has been trained on data. This means that the model has knowledge of events, legal cases, and relevant information only up to that date. Anything that happens after the cut-off day is not included in the model's training data, and therefore the model may not be aware of more recent developments, changes in laws, or new legal precedents.\n",
            "\n",
            "For example, if an LLM has a cut-off day of October 2023, it means that all the information it uses to generate responses is based on data available up until that month. If there were significant legal rulings or changes in legislation after that date, the model would not have access to that information.\n",
            "\n",
            "This is important for users to understand because it can affect the accuracy and relevance of the model's responses regarding current legal issues or recent changes in the law. It's always a good practice to verify any legal information with up-to-date sources, especially if it pertains to current events or recent cases.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "human_message = HumanMessage(content= \"Who decides the cut-off day for an LLM and how can user of the LLM overcome that?\")\n",
        "messages[1] = human_message\n",
        "response = gpt4o_mini_model.invoke(messages)\n",
        "print(response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oU6_Hcv00IyQ",
        "outputId": "665c5dc6-433b-48a1-9729-8f080ed06a8b"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The cut-off day for a Large Language Model (LLM) refers to the last date when the model was trained on data. This date is set by the developers or researchers who created the model. They compile a dataset up until a certain point and then use that data to train the model. After that cut-off, the model doesn’t have access to new information or events unless it’s updated or retrained with more recent data.\n",
            "\n",
            "### How Users Can Overcome the Cut-off Limitation:\n",
            "\n",
            "1. **Supplement with External Data**: Users can provide the LLM with recent information or context when asking questions. For instance, if you're asking about a recent event, you could summarize that event in your prompt. This gives the model the necessary context to generate relevant responses.\n",
            "\n",
            "2. **Consult Updated Resources**: Users can cross-reference the model's outputs with other up-to-date resources, such as news articles, academic papers, or databases. This is especially useful for topics that change rapidly, like technology or current events.\n",
            "\n",
            "3. **Fine-tuning**: For organizations or developers, they can fine-tune the LLM with more recent data. This involves retraining the model on new datasets that include recent events or trends, which can help the model remain relevant.\n",
            "\n",
            "4. **Use of Plugins or APIs**: Some LLMs can be integrated with plugins or APIs that fetch real-time data. For example, if the model is connected to a news API, it can pull in the latest headlines or information to provide more accurate and current responses.\n",
            "\n",
            "5. **Explicit Queries**: Users can make their questions more explicit about the time frame they’re interested in. Instead of asking, “What are the current trends?” you might ask, “What were the trends as of October 2023?” This helps the model understand the scope of the inquiry.\n",
            "\n",
            "By understanding the limitations of the cut-off date and using these strategies, users can still obtain useful and timely information from an LLM.\n"
          ]
        }
      ]
    }
  ]
}