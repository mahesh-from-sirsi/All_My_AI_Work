{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mahesh-from-sirsi/All_My_AI_Work/blob/main/Using_LLMs_with_APIs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8xgmhFrxFdZ1"
      },
      "source": [
        "## Using LLMs with APIs\n",
        "\n",
        "In this notebook, we will cover how you can use different LLMs across providers using simple APIs\n",
        "\n",
        "We will cover\n",
        "\n",
        "1. OpenAI Models\n",
        "2. Gemini Models\n",
        "3. Llama and Other Open Source models\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hza7XrQaAf0C"
      },
      "source": [
        "### Using OpenAI Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AeWgbkytAfoK"
      },
      "outputs": [],
      "source": [
        "# Installing essential library\n",
        "!pip install langchain-openai==0.2.12"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bzzamJ9UBJ9Q"
      },
      "source": [
        "Get OpenAI API key: https://platform.openai.com/account/api-keys"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6-MUWqV7Afiy"
      },
      "outputs": [],
      "source": [
        "# Saving the API key\n",
        "\n",
        "import os\n",
        "os.environ['OPENAI_API_KEY'] = \"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QhOI_FG7A5DI"
      },
      "outputs": [],
      "source": [
        "# Import ChatOpenAI module\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "# Initialize OpenAI's GPT-4o-mini\n",
        "gpt4o_mini = ChatOpenAI(model_name = \"gpt-4o-mini\")  # use \"gpt-4o\" for larger GPT-4 model\n",
        "\n",
        "gpt4o = ChatOpenAI(model_name = \"gpt-4o\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bkyDYBrZCDXm",
        "outputId": "5ae815ab-4aa9-4f18-c968-e193f1a5edce"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content='I am an AI language model created by OpenAI, designed to assist with a variety of tasks, answer questions, and provide information on a wide range of topics. How can I help you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 40, 'prompt_tokens': 11, 'total_tokens': 51, 'completion_tokens_details': {'audio_tokens': None, 'reasoning_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_f85bea6784', 'finish_reason': 'stop', 'logprobs': None}, id='run-2ad116bd-3cf2-4d3d-a95c-5ed6ff4b9118-0', usage_metadata={'input_tokens': 11, 'output_tokens': 40, 'total_tokens': 51, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 0}})"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Example of using the GPT-4o-mini model\n",
        "\n",
        "response = gpt4o_mini.invoke(\"Who are you?\")\n",
        "\n",
        "response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ldkmBZb9CNh6",
        "outputId": "53d0d9b6-2b7a-4860-fd80-548fc4580ad9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "I am an AI language model created by OpenAI, designed to assist with a variety of tasks, answer questions, and provide information on a wide range of topics. How can I help you today?\n"
          ]
        }
      ],
      "source": [
        "print(response.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ls44lin5RsYo"
      },
      "outputs": [],
      "source": [
        "response = gpt4o_mini.invoke(\"Explain how acid rains work\")\n",
        "\n",
        "response"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yG7CV0imBsat"
      },
      "source": [
        "## Using Gemini Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XEOW16KuBsFg"
      },
      "outputs": [],
      "source": [
        "# Installing Langchain's Google Gen AI library\n",
        "%pip install langchain-google-genai==2.0.1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6E4kd2j-B1-q"
      },
      "source": [
        "Get Google API key: https://aistudio.google.com (FREE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JgP7harNBzk3"
      },
      "outputs": [],
      "source": [
        "# Saving the API key\n",
        "import os\n",
        "\n",
        "os.environ['GOOGLE_API_KEY']  = \"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sShoMxF2Cgku",
        "outputId": "1fefee9f-e8d5-492a-b2a5-50023837d931"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/satvikp/Desktop/GenAI_Bootcamp/notebooks/genai-bootcamp/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "# Using Google Models (Gemini Flash)\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "# Initialize Google's Gemini model\n",
        "gemini_flash_model = ChatGoogleGenerativeAI(model = \"gemini-1.5-flash-latest\") # use \"gemini-1.5-pro-002\" for Gemini Pro model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uIgczp_eC1qi",
        "outputId": "286c9cce-43b4-4c4a-fd72-1cf12d3a7846"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "## Transformers Architecture: A Deep Dive\n",
            "\n",
            "Transformers are a type of neural network architecture that revolutionized natural language processing (NLP) and are now widely used in various fields like computer vision and time series analysis. Here's a breakdown of their architecture:\n",
            "\n",
            "**Core Components:**\n",
            "\n",
            "1. **Encoder:** This part of the transformer processes the input sequence (e.g., a sentence) to generate a representation that captures its meaning and context. It consists of multiple layers, each with two main components:\n",
            "    * **Self-Attention:** This mechanism allows the model to attend to different parts of the input sequence and understand their relationships. It calculates attention weights for each word based on its relevance to other words in the sequence.\n",
            "    * **Feedforward Neural Network (FFNN):** This network applies a non-linear transformation to the attention-weighted representations, further enriching the encoded information.\n",
            "\n",
            "2. **Decoder:** This part of the transformer takes the encoded representation as input and generates the output sequence (e.g., translation, summary, or a generated text). It also consists of multiple layers with:\n",
            "    * **Masked Self-Attention:** Similar to the encoder, this component calculates attention weights but masks future words to prevent the model from \"seeing\" words that haven't been generated yet. This ensures a coherent and sequential output.\n",
            "    * **Encoder-Decoder Attention:** This mechanism allows the decoder to attend to the encoded representation of the input sequence, providing context for the output generation.\n",
            "    * **FFNN:** Similar to the encoder, this network transforms the attention-weighted representations.\n",
            "\n",
            "**Key Concepts:**\n",
            "\n",
            "* **Attention Mechanism:** The heart of transformers, attention allows the model to focus on relevant parts of the input sequence and weigh their importance. This is crucial for understanding long-range dependencies and capturing complex relationships between words.\n",
            "* **Positional Encoding:** Since transformers don't inherently understand the order of words, positional encoding is used to provide information about the position of each word in the sequence.\n",
            "* **Multi-Head Attention:** This technique allows the model to attend to different aspects of the input sequence simultaneously, by having multiple attention heads that focus on different features.\n",
            "* **Layer Normalization:** This technique helps to stabilize the training process and prevent vanishing gradients.\n",
            "\n",
            "**Advantages of Transformers:**\n",
            "\n",
            "* **Parallel Processing:** Transformers can process entire sequences in parallel, making them significantly faster than recurrent neural networks (RNNs) which process data sequentially.\n",
            "* **Long-Range Dependencies:** They excel at capturing long-range dependencies between words, which is crucial for understanding complex language structures.\n",
            "* **High Accuracy:** Transformers have achieved state-of-the-art results in various NLP tasks, including machine translation, text summarization, question answering, and sentiment analysis.\n",
            "\n",
            "**Applications:**\n",
            "\n",
            "* **Machine Translation:** Translating text from one language to another.\n",
            "* **Text Summarization:** Generating concise summaries of long texts.\n",
            "* **Question Answering:** Answering questions based on given context.\n",
            "* **Sentiment Analysis:** Determining the emotional tone of text.\n",
            "* **Speech Recognition:** Converting speech to text.\n",
            "* **Image Captioning:** Generating descriptions for images.\n",
            "* **Time Series Forecasting:** Predicting future values of time-dependent data.\n",
            "\n",
            "**Limitations:**\n",
            "\n",
            "* **Computational Cost:** Transformers can be computationally expensive to train and deploy, especially for large models.\n",
            "* **Lack of Interpretability:** It can be difficult to understand exactly how transformers make predictions, making it challenging to debug and interpret their decisions.\n",
            "\n",
            "**Conclusion:**\n",
            "\n",
            "Transformers have revolutionized NLP and continue to push the boundaries of what's possible in AI. Their ability to capture complex relationships and process data efficiently makes them a powerful tool for various applications. However, understanding their limitations and exploring ways to improve their interpretability and computational efficiency is crucial for their continued development.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Example of using the Flash model\n",
        "\n",
        "response = gemini_flash_model.invoke(\"Explain transformers architecture.\")\n",
        "\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c1Ilgey2Dnzr"
      },
      "source": [
        "## Using Open-Source models\n",
        "\n",
        "We will demonstrate how you can use Llama, Mistral, etc using Together API.\n",
        "\n",
        "- Models by Together:  https://api.together.xyz/models\n",
        "- Get your Together API Key: https://api.together.xyz/settings/api-keys\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pRp_L-xWGJ7r",
        "outputId": "a74e3367-d102-4237-a506-3b9b11305b92"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "I'm an artificial intelligence model known as Llama. Llama stands for \"Large Language Model Meta AI.\"\n"
          ]
        }
      ],
      "source": [
        "# Using a Llama Model\n",
        "\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "llama_model = ChatOpenAI(model = \"meta-llama/Llama-3.2-90B-Vision-Instruct-Turbo\",\n",
        "                      openai_api_key = \"\", ## use your key\n",
        "                      openai_api_base = \"https://api.together.xyz/v1\"\n",
        "\n",
        ")\n",
        "\n",
        "\n",
        "response = llama_model.invoke(\"Who are you?\")\n",
        "\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p2BWtukwGbea",
        "outputId": "1859aa47-582b-4eb9-91f2-f958f2c23f2f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " I am an artificial intelligence assistant. I am designed to help answer questions, provide information, and assist with various tasks. I can help with a wide range of topics, from general knowledge to specific queries. My goal is to make your life easier by providing quick and accurate information.\n"
          ]
        }
      ],
      "source": [
        "# Using a Mixtral Model\n",
        "\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "mixtral_model = ChatOpenAI(model = \"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n",
        "                      openai_api_key = \"\", ## use your key\n",
        "                      openai_api_base = \"https://api.together.xyz/v1\")\n",
        "\n",
        "\n",
        "response = mixtral_model.invoke(\"Who are you?\")\n",
        "\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d364kWINGpes"
      },
      "outputs": [],
      "source": [
        "response = mixtral_model.invoke(\"Tell me long story\")\n",
        "\n",
        "print(response.content)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}