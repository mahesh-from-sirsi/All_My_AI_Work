{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNFshC4akPaTvuCXomSFl2u",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mahesh-from-sirsi/All_My_AI_Work/blob/main/Module3_2_1_FineTunig_LLM_ipynb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fine Tuning LLM\n",
        "\n",
        "#--------------------------------------------------------------------------\n",
        "### Here are the steps:\n",
        "\n",
        "#### Step1: Prepare the Data needed to Fine tune the LLM (We are using a Instruction Tuning Technique Here) - Here we are training the LLM to generate the LinkedIn Posts Just like I have written myself, using my style. For this we collect all my posts and added it to CSV Output Column and for each post I created a possible prompt (using GPT) and added that to the Input column:\n",
        "\n",
        "     * Create a CSV file that has two columns (Input and the Output)\n",
        "     * The Input column is a prompt that generates the post\n",
        "     * The Output column is the actual post I have posted on LinkedIn\n",
        "     * We translated the CSV file into a JSONL format\n",
        "\n",
        "#### Step2: Fine Tune the model:\n",
        "     * Open https://platform.openai.com/finetune/ftjob-0bccwfBJbaDF1agqBOE616Va?filter=all\n",
        "     * Go To Fine-Tuning > Create >\n",
        "       Provide Suffix\n",
        "       Upload the jsonl file\n",
        "     * Click Create Button\n",
        "     * This will take some time to create the FineTuned LLM\n",
        "\n",
        "#### Step3: Use the model to generate the pposts, compare the posts generated by the model and by the fine tuned model  "
      ],
      "metadata": {
        "id": "xHyL9OJ0glVT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JKrhkx1MgXzX",
        "outputId": "01dd5fc1-5a67-4c98-9f8c-457819acfb32"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/50.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.7/50.7 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -qU langchain-openai==0.2.12"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get(\"OPENAI_API_KEY\")"
      ],
      "metadata": {
        "id": "M5QmR8FQjguk"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "base_gpt = ChatOpenAI(model_name=\"gpt-4.1-mini-2025-04-14\")\n",
        "finetuned_gpt = ChatOpenAI(model_name=\"ft:gpt-4.1-mini-2025-04-14:daivajnaads:maheshvs-linkedinposts:CJZ5hLYj\")"
      ],
      "metadata": {
        "id": "bhU38QAMj3oH"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response1 = base_gpt.invoke(\"Write a LinkedIn post on Transformers Architecture\")\n",
        "response2 = finetuned_gpt.invoke(\"Write a LinkedIn post on Transformers Architecture\")\n",
        "\n",
        "print(response1.content)\n",
        "print(\"=======================\\n\")\n",
        "print(response2.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VUyfOw7WkG_A",
        "outputId": "82434131-c846-4a28-f67e-1ce3bbd7a1de"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🚀 Exploring the Power of Transformers Architecture in AI 🚀\n",
            "\n",
            "Transformers have revolutionized the field of artificial intelligence and natural language processing since their introduction in 2017. Unlike traditional sequential models, Transformers leverage self-attention mechanisms to process data in parallel, enabling them to capture long-range dependencies and context more effectively.\n",
            "\n",
            "Key highlights of Transformer architecture:\n",
            "✨ **Self-Attention:** Allows the model to weigh the importance of different words in a sentence, regardless of their position.\n",
            "✨ **Parallel Processing:** Enables faster training and better scalability compared to RNNs and LSTMs.\n",
            "✨ **Versatility:** Beyond NLP, Transformers are now powering breakthroughs in computer vision, speech recognition, and even protein folding.\n",
            "\n",
            "This architecture underpins cutting-edge models like BERT, GPT, and many others, driving advancements in language understanding, generation, and more.\n",
            "\n",
            "As AI continues to evolve, Transformers remain at the forefront, shaping the future of intelligent systems. Whether you're a researcher, developer, or enthusiast, understanding Transformers is key to unlocking the next wave of innovation.\n",
            "\n",
            "#AI #MachineLearning #DeepLearning #Transformers #NLP #Innovation #TechTrends\n",
            "=======================\n",
            "\n",
            "Transformers Architecture has revolutionized the field of artificial intelligence and natural language processing. From chatbots to code generation, the impact of transformers is profound and far-reaching.\n",
            "\n",
            "At its core, the Transformer model leverages self-attention mechanisms to process input data, allowing it to weigh the importance of different words or tokens relative to each other. This enables the model to capture context and relationships more effectively than traditional recurrent or convolutional neural networks.\n",
            "\n",
            "Key components of the Transformer architecture include:\n",
            "- Multi-head self-attention, which allows the model to attend to information from different representation subspaces.\n",
            "- Positional encoding, which injects information about the order of tokens, crucial since the model processes input in parallel.\n",
            "- Feed-forward neural networks applied to each position separately and identically.\n",
            "- Residual connections and layer normalization, which facilitate training deep networks.\n",
            "\n",
            "Transformers have paved the way for large language models like GPT, BERT, and T5, enabling applications ranging from translation to reasoning.\n",
            "\n",
            "For those interested in the technical details, I highly recommend reading the original paper, “Attention Is All You Need,” available at https://lnkd.in/e5Fqc2F7\n"
          ]
        }
      ]
    }
  ]
}